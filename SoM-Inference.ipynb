{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 07:12:05.522967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741504325.540271    4832 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741504325.545586    4832 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-09 07:12:05.563034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/venv/main/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the processor and model, then move the model to the chosen device\u001b[39;00m\n\u001b[1;32m     10\u001b[0m processor \u001b[38;5;241m=\u001b[39m Pix2StructProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/deplot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPix2StructForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoogle/deplot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor and model, then move the model to the chosen device\n",
    "processor = Pix2StructProcessor.from_pretrained('google/deplot')\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://t4.ftcdn.net/jpg/01/62/69/25/360_F_162692511_SidIKVCDnt5UKHPNqpCb2MSKvfBlx1lG.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m      3\u001b[0m image\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://t4.ftcdn.net/jpg/01/62/69/25/360_F_162692511_SidIKVCDnt5UKHPNqpCb2MSKvfBlx1lG.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, If both images and text are provided, image_processor is not a VQA processor, and `add_special_tokens` is unset, the default value of `add_special_tokens` will be changed to `False` when calling the tokenizer. To test the new behavior, set `legacy=False`as a processor call argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare inputs and move them to the device\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate underlying data table of the figure below:\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: tensor\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate predictions on the GPU\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare inputs and move them to the device\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate underlying data table of the figure below:\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate predictions on the GPU\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare inputs and move them to the device\n",
    "inputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\n",
    "inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "# Generate predictions on the GPU\n",
    "predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# Decode and print the output\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import  display\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "import cv2\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"MMMU/MMMU_Pro\", \"vision\")\n",
    "\n",
    "print(ds)\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the image\n",
    "image = ds[\"test\"][0][\"image\"]\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")  # Remove axes\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Kosmos2ForConditionalGeneration\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the Kosmos-2 model and processor, move the model to the GPU\n",
    "model_kosmos = Kosmos2ForConditionalGeneration.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(device)\n",
    "# model_kosmos = Kosmos2ForConditionalGeneration.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(device)\n",
    "processor_kosmos = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_caption_and_entities_kosmos_2(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = \"<grounding> An image of\"\n",
    "\n",
    "    # Process the input for the model\n",
    "    inputs = processor_kosmos(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Move tensors to GPU if available\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "\n",
    "    # Generate the output\n",
    "    generated_ids = model_kosmos.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        use_cache=True,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = processor_kosmos.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    processed_text = processor_kosmos.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "    # Extract caption and entities\n",
    "    caption, entities = processor_kosmos.post_process_generation(generated_text)\n",
    "\n",
    "    return caption, entities\n",
    "\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def apply_bbox_mask(image_path, entities):\n",
    "    \"\"\"\n",
    "    Applies a filter mask to an image, keeping only the pixels within the bounding boxes and blackening the rest.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        entities (list): List of tuples containing object descriptions and bounding box information.\n",
    "                         Each tuple should be in the format:\n",
    "                         (description, text_range, [bounding_box_list]),\n",
    "                         where bounding_box_list contains relative coordinates (x_min, y_min, x_max, y_max).\n",
    "\n",
    "    Returns:\n",
    "        Image: Processed image with pixels outside the bounding boxes blackened.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Create a mask with the same size as the image\n",
    "    mask = Image.new(\"L\", image.size, 0)  # \"L\" mode for grayscale\n",
    "\n",
    "    # Draw the bounding boxes on the mask\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    for _, _, bboxes in entities:\n",
    "        for bbox in bboxes:\n",
    "            # Convert bbox relative coordinates to pixel coordinates\n",
    "            x_min = int(bbox[0] * image.width)\n",
    "            y_min = int(bbox[1] * image.height)\n",
    "            x_max = int(bbox[2] * image.width)\n",
    "            y_max = int(bbox[3] * image.height)\n",
    "            # Draw a white rectangle for the bounding box\n",
    "            draw.rectangle([x_min, y_min, x_max, y_max], fill=255)\n",
    "\n",
    "    # Apply the mask to the image\n",
    "    filtered_image = Image.new(\"RGB\", image.size)\n",
    "    filtered_image.paste(image, mask=mask)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Specify the directory to remove\n",
    "# directory = \"/content/Depth-Anything-V2\"\n",
    "\n",
    "# # Check if the directory exists before attempting to remove it\n",
    "# if os.path.exists(directory):\n",
    "#     shutil.rmtree(directory)\n",
    "#     print(f\"Directory '{directory}' has been removed successfully.\")\n",
    "# else:\n",
    "#     print(f\"Directory '{directory}' does not exist.\")\n",
    "\n",
    "\n",
    "# Depth anything\n",
    "\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/spaces/depth-anything/Depth-Anything-V2\n",
    "\n",
    "# !GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/spaces/depth-anything/Depth-Anything-V2\n",
    "\n",
    "!pip install -r /content/Depth-Anything-V2/requirements.txt\n",
    "!pip install spaces\n",
    "\n",
    "import os\n",
    "\n",
    "# Change to the desired directory\n",
    "os.chdir(\"/content/Depth-Anything-V2\")\n",
    "\n",
    "# Verify the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "\n",
    "## Now\n",
    "\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import spaces\n",
    "import torch\n",
    "import tempfile\n",
    "from gradio_imageslider import ImageSlider\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "#img-display-container {\n",
    "    max-height: 100vh;\n",
    "}\n",
    "#img-display-input {\n",
    "    max-height: 80vh;\n",
    "}\n",
    "#img-display-output {\n",
    "    max-height: 80vh;\n",
    "}\n",
    "#download {\n",
    "    height: 62px;\n",
    "}\n",
    "\"\"\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "encoder2name = {\n",
    "    'vits': 'Small',\n",
    "    'vitb': 'Base',\n",
    "    'vitl': 'Large',\n",
    "    'vitg': 'Giant', # we are undergoing company review procedures to release our giant model checkpoint\n",
    "}\n",
    "encoder = 'vitl'\n",
    "model_name = encoder2name[encoder]\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "filepath = hf_hub_download(repo_id=f\"depth-anything/Depth-Anything-V2-{model_name}\", filename=f\"depth_anything_v2_{encoder}.pth\", repo_type=\"model\")\n",
    "state_dict = torch.load(filepath, map_location=\"cpu\",weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(DEVICE).eval()\n",
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "\n",
    "\n",
    "title = \"# Depth Anything V2\"\n",
    "description = \"\"\"Official demo for **Depth Anything V2**.\n",
    "Please refer to our [paper](https://arxiv.org/abs/2406.09414), [project page](https://depth-anything-v2.github.io), and [github](https://github.com/DepthAnything/Depth-Anything-V2) for more details.\"\"\"\n",
    "\n",
    "@spaces.GPU\n",
    "def predict_depth(image):\n",
    "    return model.infer_image(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_submit(image):\n",
    "    original_image = image.copy()\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    depth = predict_depth(image[:, :, ::-1])\n",
    "\n",
    "    raw_depth = Image.fromarray(depth.astype('uint16'))\n",
    "    tmp_raw_depth = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "    raw_depth.save(tmp_raw_depth.name)\n",
    "\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    depth = depth.astype(np.uint8)\n",
    "    colored_depth = (cmap(depth)[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    gray_depth = Image.fromarray(depth)\n",
    "    tmp_gray_depth = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "    gray_depth.save(tmp_gray_depth.name)\n",
    "\n",
    "    return [(original_image, colored_depth), tmp_gray_depth.name, tmp_raw_depth.name]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open(\"/content/image.jpg\")\n",
    "\n",
    "def make_depth_context_img(image):\n",
    "    # Convert to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    print(\"Image loaded as NumPy array:\")\n",
    "    print(image_array.shape)\n",
    "\n",
    "    all_outputs = on_submit(image_array)\n",
    "\n",
    "    # Assuming image_array is the original image array and all_outputs[0][1] contains the intensity values\n",
    "    intensity_image = all_outputs[0][1]  # Image from which we calculate intensity percentiles\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width, channels = image_array.shape\n",
    "\n",
    "    # Flatten the intensity image to get a list of all pixel intensities\n",
    "    flattened_intensity_image = intensity_image.reshape(-1, 3)\n",
    "\n",
    "    # Calculate intensity values for each pixel (using the average of RGB channels)\n",
    "    intensities = np.mean(flattened_intensity_image, axis=1)\n",
    "\n",
    "    # Calculate the percentiles\n",
    "    top30_threshold = np.percentile(intensities, 70)  # 70th percentile for top 30%\n",
    "    bottom30_threshold = np.percentile(intensities, 30)  # 30th percentile for bottom 30%\n",
    "    mid40_lower_threshold = np.percentile(intensities, 30)  # 30th percentile\n",
    "    mid40_upper_threshold = np.percentile(intensities, 70)  # 70th percentile\n",
    "\n",
    "    # Create masks based on the intensity thresholds\n",
    "    top30_mask_flat = intensities > top30_threshold\n",
    "    bottom30_mask_flat = intensities < bottom30_threshold\n",
    "    mid40_mask_flat = (intensities >= mid40_lower_threshold) & (intensities <= mid40_upper_threshold)\n",
    "\n",
    "    # Reshape masks to match the original image shape (height, width, channels)\n",
    "    top30_mask = top30_mask_flat.reshape(height, width, 1)\n",
    "    bottom30_mask = bottom30_mask_flat.reshape(height, width, 1)\n",
    "    mid40_mask = mid40_mask_flat.reshape(height, width, 1)\n",
    "\n",
    "    # Apply masks to the image_array (preserve the regions and set others to black)\n",
    "    top30_image = np.where(top30_mask, image_array, 0)\n",
    "    bottom30_image = np.where(bottom30_mask, image_array, 0)\n",
    "    mid40_image = np.where(mid40_mask, image_array, 0)\n",
    "\n",
    "\n",
    "    return [top30_image, bottom30_image, mid40_image]\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = make_depth_context_img(image)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "titles = ['Top 30%', 'Bottom 30%', 'Mid 40%']\n",
    "# images = [top30_image, bottom30_image, mid40_image]\n",
    "\n",
    "for i, (img, title) in enumerate(zip(images, titles)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_caption_and_entities_kosmos_2(image_array):\n",
    "    # Convert the numpy array image to a PIL Image\n",
    "    image = Image.fromarray(image_array.astype('uint8'))\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = \"<grounding> An image of\"\n",
    "\n",
    "    # Process the input for the model\n",
    "    inputs = processor_kosmos(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Move tensors to GPU if available\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "    # Generate the output\n",
    "    generated_ids = model_kosmos.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        use_cache=True,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = processor_kosmos.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    processed_text = processor_kosmos.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "    # Extract caption and entities\n",
    "    caption, entities = processor_kosmos.post_process_generation(generated_text)\n",
    "\n",
    "    return caption, entities\n",
    "\n",
    "\n",
    "location = [\"Closest\",\"Mid Range\",\"Farthest\"]\n",
    "\n",
    "for i in range(3):\n",
    "    image = images[i]\n",
    "    entities = get_caption_and_entities_kosmos_2(image)\n",
    "\n",
    "    print(f\"{location[i]}: {entities[0]}\\n----\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Whole code\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open(\"/content/image.jpg\")\n",
    "\n",
    "def make_depth_context_img(image):\n",
    "    # Convert to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    print(\"Image loaded as NumPy array:\")\n",
    "    print(image_array.shape)\n",
    "\n",
    "    all_outputs = on_submit(image_array)\n",
    "\n",
    "    # Assuming image_array is the original image array and all_outputs[0][1] contains the intensity values\n",
    "    intensity_image = all_outputs[0][1]  # Image from which we calculate intensity percentiles\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width, channels = image_array.shape\n",
    "\n",
    "    # Flatten the intensity image to get a list of all pixel intensities\n",
    "    flattened_intensity_image = intensity_image.reshape(-1, 3)\n",
    "\n",
    "    # Calculate intensity values for each pixel (using the average of RGB channels)\n",
    "    intensities = np.mean(flattened_intensity_image, axis=1)\n",
    "\n",
    "    # Calculate the percentiles\n",
    "    top30_threshold = np.percentile(intensities, 70)  # 70th percentile for top 30%\n",
    "    bottom30_threshold = np.percentile(intensities, 30)  # 30th percentile for bottom 30%\n",
    "    mid40_lower_threshold = np.percentile(intensities, 30)  # 30th percentile\n",
    "    mid40_upper_threshold = np.percentile(intensities, 70)  # 70th percentile\n",
    "\n",
    "    # Create masks based on the intensity thresholds\n",
    "    top30_mask_flat = intensities > top30_threshold\n",
    "    bottom30_mask_flat = intensities < bottom30_threshold\n",
    "    mid40_mask_flat = (intensities >= mid40_lower_threshold) & (intensities <= mid40_upper_threshold)\n",
    "\n",
    "    # Reshape masks to match the original image shape (height, width, channels)\n",
    "    top30_mask = top30_mask_flat.reshape(height, width, 1)\n",
    "    bottom30_mask = bottom30_mask_flat.reshape(height, width, 1)\n",
    "    mid40_mask = mid40_mask_flat.reshape(height, width, 1)\n",
    "\n",
    "    # Apply masks to the image_array (preserve the regions and set others to black)\n",
    "    top30_image = np.where(top30_mask, image_array, 0)\n",
    "    bottom30_image = np.where(bottom30_mask, image_array, 0)\n",
    "    mid40_image = np.where(mid40_mask, image_array, 0)\n",
    "\n",
    "\n",
    "    return [top30_image, bottom30_image, mid40_image]\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = make_depth_context_img(image)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "titles = ['Top 30%', 'Bottom 30%', 'Mid 40%']\n",
    "# images = [top30_image, bottom30_image, mid40_image]\n",
    "\n",
    "\n",
    "for i, (img, title) in enumerate(zip(images, titles)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_caption_and_entities_kosmos_2(image_array):\n",
    "    # Convert the numpy array image to a PIL Image\n",
    "    image = Image.fromarray(image_array.astype('uint8'))\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = \"<grounding> An image of\"\n",
    "\n",
    "    # Process the input for the model\n",
    "    inputs = processor_kosmos(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Move tensors to GPU if available\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "    # Generate the output\n",
    "    generated_ids = model_kosmos.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        use_cache=True,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = processor_kosmos.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    processed_text = processor_kosmos.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "    # Extract caption and entities\n",
    "    caption, entities = processor_kosmos.post_process_generation(generated_text)\n",
    "\n",
    "    return caption, entities\n",
    "\n",
    "\n",
    "\n",
    "location = [\"Closest\",\"Mid Range\",\"Farthest\"]\n",
    "\n",
    "for i in range(3):\n",
    "    image = images[i]\n",
    "    entities = get_caption_and_entities_kosmos_2(image)\n",
    "\n",
    "    print(f\"{location[i]}: {entities[0]}\\n----\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
