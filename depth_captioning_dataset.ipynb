{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch torchvision torchaudio --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /root/Depth-SoM/Depth-Anything-V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n",
      "2025-04-04 18:13:42.149439: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 18:13:42.164403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743790422.181899    1979 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743790422.187167    1979 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743790422.200269    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743790422.200283    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743790422.200284    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743790422.200286    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 18:13:42.206135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c313e9f00054871817e54935ab35595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/534 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e7ef944b7b4898a018e10ebe818252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/191k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2e98fa63b0442991860c85546cb831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef5446356a94aca82b2b723b9738572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef47c55217d420e93a04f692a361367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b2241bd2774efaa5b4c6095309e0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc161ec46884ee092dfc737e08135d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65558fac222248198c5f940202e4edde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2154f869d5449cb0d7fb7a12604e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eea629e5bd74627bf2f5d2c6e9c74ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "depth_anything_v2_vitl.pth:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from depth_kosmos import requests, DepthKosmosCaptioner\n",
    "from depth_kosmos import Image as Image_PIL\n",
    "\n",
    "depth_kosmos_captioner = DepthKosmosCaptioner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTp5jMLHnfiO56w8iVWAwI4VvOu4B_5c2C1ww&s\"\n",
    "image = Image_PIL.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest: a woman carrying a basket of plants\n",
      "----\n",
      "Mid Range: a soldier\n",
      "----\n",
      "Farthest: a woman\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# depth_kosmos_captioner.display_depth_images(image)\n",
    "full_caption_string = depth_kosmos_captioner.get_caption_with_depth(image)\n",
    "print(full_caption_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd46709e0a0c4081bfc2b90e37a1249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0461827a9b4298b6b87a1b7449479e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/60.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76736f52a2544a0fa7b633b35d2b0ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: image/image_0.jpg\n",
      "✅ Saved: image/image_1.jpg\n",
      "✅ Saved: image/image_2.jpg\n",
      "✅ Saved: image/image_3.jpg\n",
      "✅ Saved: image/image_4.jpg\n",
      "✅ Saved: image/image_5.jpg\n",
      "✅ Saved: image/image_6.jpg\n",
      "✅ Saved: image/image_7.jpg\n",
      "✅ Saved: image/image_8.jpg\n",
      "✅ Saved: image/image_9.jpg\n",
      "✅ Saved: image/image_10.jpg\n",
      "✅ Saved: image/image_11.jpg\n",
      "✅ Saved: image/image_12.jpg\n",
      "✅ Saved: image/image_13.jpg\n",
      "✅ Saved: image/image_14.jpg\n",
      "✅ Saved: image/image_15.jpg\n",
      "✅ Saved: image/image_16.jpg\n",
      "✅ Saved: image/image_17.jpg\n",
      "✅ Saved: image/image_18.jpg\n",
      "✅ Saved: image/image_19.jpg\n",
      "✅ Saved: image/image_20.jpg\n",
      "✅ Saved: image/image_21.jpg\n",
      "✅ Saved: image/image_22.jpg\n",
      "✅ Saved: image/image_23.jpg\n",
      "✅ Saved: image/image_24.jpg\n",
      "✅ Saved: image/image_25.jpg\n",
      "✅ Saved: image/image_26.jpg\n",
      "✅ Saved: image/image_27.jpg\n",
      "✅ Saved: image/image_28.jpg\n",
      "✅ Saved: image/image_29.jpg\n",
      "✅ Saved: image/image_30.jpg\n",
      "✅ Saved: image/image_31.jpg\n",
      "✅ Saved: image/image_32.jpg\n",
      "✅ Saved: image/image_33.jpg\n",
      "✅ Saved: image/image_34.jpg\n",
      "✅ Saved: image/image_35.jpg\n",
      "✅ Saved: image/image_36.jpg\n",
      "✅ Saved: image/image_37.jpg\n",
      "✅ Saved: image/image_38.jpg\n",
      "✅ Saved: image/image_39.jpg\n",
      "✅ Saved: image/image_40.jpg\n",
      "✅ Saved: image/image_41.jpg\n",
      "✅ Saved: image/image_42.jpg\n",
      "✅ Saved: image/image_43.jpg\n",
      "✅ Saved: image/image_44.jpg\n",
      "✅ Saved: image/image_45.jpg\n",
      "✅ Saved: image/image_46.jpg\n",
      "✅ Saved: image/image_47.jpg\n",
      "✅ Saved: image/image_48.jpg\n",
      "✅ Saved: image/image_49.jpg\n",
      "✅ Saved: image/image_50.jpg\n",
      "✅ Saved: image/image_51.jpg\n",
      "✅ Saved: image/image_52.jpg\n",
      "✅ Saved: image/image_53.jpg\n",
      "✅ Saved: image/image_54.jpg\n",
      "✅ Saved: image/image_55.jpg\n",
      "✅ Saved: image/image_56.jpg\n",
      "✅ Saved: image/image_57.jpg\n",
      "✅ Saved: image/image_58.jpg\n",
      "✅ Saved: image/image_59.jpg\n",
      "✅ Saved: image/image_60.jpg\n",
      "✅ Saved: image/image_61.jpg\n",
      "✅ Saved: image/image_62.jpg\n",
      "✅ Saved: image/image_63.jpg\n",
      "✅ Saved: image/image_64.jpg\n",
      "✅ Saved: image/image_65.jpg\n",
      "✅ Saved: image/image_66.jpg\n",
      "✅ Saved: image/image_67.jpg\n",
      "✅ Saved: image/image_68.jpg\n",
      "✅ Saved: image/image_69.jpg\n",
      "✅ Saved: image/image_70.jpg\n",
      "✅ Saved: image/image_71.jpg\n",
      "✅ Saved: image/image_72.jpg\n",
      "✅ Saved: image/image_73.jpg\n",
      "✅ Saved: image/image_74.jpg\n",
      "✅ Saved: image/image_75.jpg\n",
      "✅ Saved: image/image_76.jpg\n",
      "✅ Saved: image/image_77.jpg\n",
      "✅ Saved: image/image_78.jpg\n",
      "✅ Saved: image/image_79.jpg\n",
      "✅ Saved: image/image_80.jpg\n",
      "✅ Saved: image/image_81.jpg\n",
      "✅ Saved: image/image_82.jpg\n",
      "✅ Saved: image/image_83.jpg\n",
      "✅ Saved: image/image_84.jpg\n",
      "✅ Saved: image/image_85.jpg\n",
      "✅ Saved: image/image_86.jpg\n",
      "✅ Saved: image/image_87.jpg\n",
      "✅ Saved: image/image_88.jpg\n",
      "✅ Saved: image/image_89.jpg\n",
      "✅ Saved: image/image_90.jpg\n",
      "✅ Saved: image/image_91.jpg\n",
      "✅ Saved: image/image_92.jpg\n",
      "✅ Saved: image/image_93.jpg\n",
      "✅ Saved: image/image_94.jpg\n",
      "✅ Saved: image/image_95.jpg\n",
      "✅ Saved: image/image_96.jpg\n",
      "✅ Saved: image/image_97.jpg\n",
      "✅ Saved: image/image_98.jpg\n",
      "✅ Saved: image/image_99.jpg\n",
      "✅ Saved: image/image_100.jpg\n",
      "✅ Saved: image/image_101.jpg\n",
      "✅ Saved: image/image_102.jpg\n",
      "✅ Saved: image/image_103.jpg\n",
      "✅ Saved: image/image_104.jpg\n",
      "✅ Saved: image/image_105.jpg\n",
      "✅ Saved: image/image_106.jpg\n",
      "✅ Saved: image/image_107.jpg\n",
      "✅ Saved: image/image_108.jpg\n",
      "✅ Saved: image/image_109.jpg\n",
      "✅ Saved: image/image_110.jpg\n",
      "✅ Saved: image/image_111.jpg\n",
      "✅ Saved: image/image_112.jpg\n",
      "✅ Saved: image/image_113.jpg\n",
      "✅ Saved: image/image_114.jpg\n",
      "✅ Saved: image/image_115.jpg\n",
      "✅ Saved: image/image_116.jpg\n",
      "✅ Saved: image/image_117.jpg\n",
      "✅ Saved: image/image_118.jpg\n",
      "✅ Saved: image/image_119.jpg\n",
      "✅ Saved: image/image_120.jpg\n",
      "✅ Saved: image/image_121.jpg\n",
      "✅ Saved: image/image_122.jpg\n",
      "✅ Saved: image/image_123.jpg\n",
      "✅ Saved: image/image_124.jpg\n",
      "✅ Saved: image/image_125.jpg\n",
      "✅ Saved: image/image_126.jpg\n",
      "✅ Saved: image/image_127.jpg\n",
      "✅ Saved: image/image_128.jpg\n",
      "✅ Saved: image/image_129.jpg\n",
      "✅ Saved: image/image_130.jpg\n",
      "✅ Saved: image/image_131.jpg\n",
      "✅ Saved: image/image_132.jpg\n",
      "✅ Saved: image/image_133.jpg\n",
      "✅ Saved: image/image_134.jpg\n",
      "✅ Saved: image/image_135.jpg\n",
      "✅ Saved: image/image_136.jpg\n",
      "✅ Saved: image/image_137.jpg\n",
      "✅ Saved: image/image_138.jpg\n",
      "✅ Saved: image/image_139.jpg\n",
      "✅ Saved: image/image_140.jpg\n",
      "✅ Saved: image/image_141.jpg\n",
      "✅ Saved: image/image_142.jpg\n",
      "✅ Saved: image/image_143.jpg\n",
      "✅ Saved: image/image_144.jpg\n",
      "✅ Saved: image/image_145.jpg\n",
      "✅ Saved: image/image_146.jpg\n",
      "✅ Saved: image/image_147.jpg\n",
      "✅ Saved: image/image_148.jpg\n",
      "✅ Saved: image/image_149.jpg\n",
      "✅ Saved: image/image_150.jpg\n",
      "✅ Saved: image/image_151.jpg\n",
      "✅ Saved: image/image_152.jpg\n",
      "✅ Saved: image/image_153.jpg\n",
      "✅ Saved: image/image_154.jpg\n",
      "✅ Saved: image/image_155.jpg\n",
      "✅ Saved: image/image_156.jpg\n",
      "✅ Saved: image/image_157.jpg\n",
      "✅ Saved: image/image_158.jpg\n",
      "✅ Saved: image/image_159.jpg\n",
      "✅ Saved: image/image_160.jpg\n",
      "✅ Saved: image/image_161.jpg\n",
      "✅ Saved: image/image_162.jpg\n",
      "✅ Saved: image/image_163.jpg\n",
      "✅ Saved: image/image_164.jpg\n",
      "✅ Saved: image/image_165.jpg\n",
      "✅ Saved: image/image_166.jpg\n",
      "✅ Saved: image/image_167.jpg\n",
      "✅ Saved: image/image_168.jpg\n",
      "✅ Saved: image/image_169.jpg\n",
      "✅ Saved: image/image_170.jpg\n",
      "✅ Saved: image/image_171.jpg\n",
      "✅ Saved: image/image_172.jpg\n",
      "✅ Saved: image/image_173.jpg\n",
      "✅ Saved: image/image_174.jpg\n",
      "✅ Saved: image/image_175.jpg\n",
      "✅ Saved: image/image_176.jpg\n",
      "✅ Saved: image/image_177.jpg\n",
      "✅ Saved: image/image_178.jpg\n",
      "✅ Saved: image/image_179.jpg\n",
      "✅ Saved: image/image_180.jpg\n",
      "✅ Saved: image/image_181.jpg\n",
      "✅ Saved: image/image_182.jpg\n",
      "✅ Saved: image/image_183.jpg\n",
      "✅ Saved: image/image_184.jpg\n",
      "✅ Saved: image/image_185.jpg\n",
      "✅ Saved: image/image_186.jpg\n",
      "✅ Saved: image/image_187.jpg\n",
      "✅ Saved: image/image_188.jpg\n",
      "✅ Saved: image/image_189.jpg\n",
      "✅ Saved: image/image_190.jpg\n",
      "✅ Saved: image/image_191.jpg\n",
      "✅ Saved: image/image_192.jpg\n",
      "✅ Saved: image/image_193.jpg\n",
      "✅ Saved: image/image_194.jpg\n",
      "✅ Saved: image/image_195.jpg\n",
      "✅ Saved: image/image_196.jpg\n",
      "✅ Saved: image/image_197.jpg\n",
      "✅ Saved: image/image_198.jpg\n",
      "✅ Saved: image/image_199.jpg\n",
      "✅ Saved: image/image_200.jpg\n",
      "✅ Saved: image/image_201.jpg\n",
      "✅ Saved: image/image_202.jpg\n",
      "✅ Saved: image/image_203.jpg\n",
      "✅ Saved: image/image_204.jpg\n",
      "✅ Saved: image/image_205.jpg\n",
      "✅ Saved: image/image_206.jpg\n",
      "✅ Saved: image/image_207.jpg\n",
      "✅ Saved: image/image_208.jpg\n",
      "✅ Saved: image/image_209.jpg\n",
      "✅ Saved: image/image_210.jpg\n",
      "✅ Saved: image/image_211.jpg\n",
      "✅ Saved: image/image_212.jpg\n",
      "✅ Saved: image/image_213.jpg\n",
      "✅ Saved: image/image_214.jpg\n",
      "✅ Saved: image/image_215.jpg\n",
      "✅ Saved: image/image_216.jpg\n",
      "✅ Saved: image/image_217.jpg\n",
      "✅ Saved: image/image_218.jpg\n",
      "✅ Saved: image/image_219.jpg\n",
      "✅ Saved: image/image_220.jpg\n",
      "✅ Saved: image/image_221.jpg\n",
      "✅ Saved: image/image_222.jpg\n",
      "✅ Saved: image/image_223.jpg\n",
      "✅ Saved: image/image_224.jpg\n",
      "✅ Saved: image/image_225.jpg\n",
      "✅ Saved: image/image_226.jpg\n",
      "✅ Saved: image/image_227.jpg\n",
      "✅ Saved: image/image_228.jpg\n",
      "✅ Saved: image/image_229.jpg\n",
      "✅ Saved: image/image_230.jpg\n",
      "✅ Saved: image/image_231.jpg\n",
      "✅ Saved: image/image_232.jpg\n",
      "✅ Saved: image/image_233.jpg\n",
      "✅ Saved: image/image_234.jpg\n",
      "✅ Saved: image/image_235.jpg\n",
      "✅ Saved: image/image_236.jpg\n",
      "✅ Saved: image/image_237.jpg\n",
      "✅ Saved: image/image_238.jpg\n",
      "✅ Saved: image/image_239.jpg\n",
      "✅ Saved: image/image_240.jpg\n",
      "✅ Saved: image/image_241.jpg\n",
      "✅ Saved: image/image_242.jpg\n",
      "✅ Saved: image/image_243.jpg\n",
      "✅ Saved: image/image_244.jpg\n",
      "✅ Saved: image/image_245.jpg\n",
      "✅ Saved: image/image_246.jpg\n",
      "✅ Saved: image/image_247.jpg\n",
      "✅ Saved: image/image_248.jpg\n",
      "✅ Saved: image/image_249.jpg\n",
      "✅ Saved: image/image_250.jpg\n",
      "✅ Saved: image/image_251.jpg\n",
      "✅ Saved: image/image_252.jpg\n",
      "✅ Saved: image/image_253.jpg\n",
      "✅ Saved: image/image_254.jpg\n",
      "✅ Saved: image/image_255.jpg\n",
      "✅ Saved: image/image_256.jpg\n",
      "✅ Saved: image/image_257.jpg\n",
      "✅ Saved: image/image_258.jpg\n",
      "✅ Saved: image/image_259.jpg\n",
      "✅ Saved: image/image_260.jpg\n",
      "✅ Saved: image/image_261.jpg\n",
      "✅ Saved: image/image_262.jpg\n",
      "✅ Saved: image/image_263.jpg\n",
      "✅ Saved: image/image_264.jpg\n",
      "✅ Saved: image/image_265.jpg\n",
      "✅ Saved: image/image_266.jpg\n",
      "✅ Saved: image/image_267.jpg\n",
      "✅ Saved: image/image_268.jpg\n",
      "✅ Saved: image/image_269.jpg\n",
      "✅ Saved: image/image_270.jpg\n",
      "✅ Saved: image/image_271.jpg\n",
      "✅ Saved: image/image_272.jpg\n",
      "✅ Saved: image/image_273.jpg\n",
      "✅ Saved: image/image_274.jpg\n",
      "✅ Saved: image/image_275.jpg\n",
      "✅ Saved: image/image_276.jpg\n",
      "✅ Saved: image/image_277.jpg\n",
      "✅ Saved: image/image_278.jpg\n",
      "✅ Saved: image/image_279.jpg\n",
      "✅ Saved: image/image_280.jpg\n",
      "✅ Saved: image/image_281.jpg\n",
      "✅ Saved: image/image_282.jpg\n",
      "✅ Saved: image/image_283.jpg\n",
      "✅ Saved: image/image_284.jpg\n",
      "✅ Saved: image/image_285.jpg\n",
      "✅ Saved: image/image_286.jpg\n",
      "✅ Saved: image/image_287.jpg\n",
      "✅ Saved: image/image_288.jpg\n",
      "✅ Saved: image/image_289.jpg\n",
      "✅ Saved: image/image_290.jpg\n",
      "✅ Saved: image/image_291.jpg\n",
      "✅ Saved: image/image_292.jpg\n",
      "✅ Saved: image/image_293.jpg\n",
      "✅ Saved: image/image_294.jpg\n",
      "✅ Saved: image/image_295.jpg\n",
      "✅ Saved: image/image_296.jpg\n",
      "✅ Saved: image/image_297.jpg\n",
      "✅ Saved: image/image_298.jpg\n",
      "✅ Saved: image/image_299.jpg\n",
      "✅ Saved: image/image_300.jpg\n",
      "✅ Saved: image/image_301.jpg\n",
      "✅ Saved: image/image_302.jpg\n",
      "✅ Saved: image/image_303.jpg\n",
      "✅ Saved: image/image_304.jpg\n",
      "✅ Saved: image/image_305.jpg\n",
      "✅ Saved: image/image_306.jpg\n",
      "✅ Saved: image/image_307.jpg\n",
      "✅ Saved: image/image_308.jpg\n",
      "✅ Saved: image/image_309.jpg\n",
      "✅ Saved: image/image_310.jpg\n",
      "✅ Saved: image/image_311.jpg\n",
      "✅ Saved: image/image_312.jpg\n",
      "✅ Saved: image/image_313.jpg\n",
      "✅ Saved: image/image_314.jpg\n",
      "✅ Saved: image/image_315.jpg\n",
      "✅ Saved: image/image_316.jpg\n",
      "✅ Saved: image/image_317.jpg\n",
      "✅ Saved: image/image_318.jpg\n",
      "✅ Saved: image/image_319.jpg\n",
      "✅ Saved: image/image_320.jpg\n",
      "✅ Saved: image/image_321.jpg\n",
      "✅ Saved: image/image_322.jpg\n",
      "✅ Saved: image/image_323.jpg\n",
      "✅ Saved: image/image_324.jpg\n",
      "✅ Saved: image/image_325.jpg\n",
      "✅ Saved: image/image_326.jpg\n",
      "✅ Saved: image/image_327.jpg\n",
      "✅ Saved: image/image_328.jpg\n",
      "✅ Saved: image/image_329.jpg\n",
      "✅ Saved: image/image_330.jpg\n",
      "✅ Saved: image/image_331.jpg\n",
      "✅ Saved: image/image_332.jpg\n",
      "✅ Saved: image/image_333.jpg\n",
      "✅ Saved: image/image_334.jpg\n",
      "✅ Saved: image/image_335.jpg\n",
      "✅ Saved: image/image_336.jpg\n",
      "✅ Saved: image/image_337.jpg\n",
      "✅ Saved: image/image_338.jpg\n",
      "✅ Saved: image/image_339.jpg\n",
      "✅ Saved: image/image_340.jpg\n",
      "✅ Saved: image/image_341.jpg\n",
      "✅ Saved: image/image_342.jpg\n",
      "✅ Saved: image/image_343.jpg\n",
      "✅ Saved: image/image_344.jpg\n",
      "✅ Saved: image/image_345.jpg\n",
      "✅ Saved: image/image_346.jpg\n",
      "✅ Saved: image/image_347.jpg\n",
      "✅ Saved: image/image_348.jpg\n",
      "✅ Saved: image/image_349.jpg\n",
      "✅ Saved: image/image_350.jpg\n",
      "✅ Saved: image/image_351.jpg\n",
      "✅ Saved: image/image_352.jpg\n",
      "✅ Saved: image/image_353.jpg\n",
      "✅ Saved: image/image_354.jpg\n",
      "✅ Saved: image/image_355.jpg\n",
      "✅ Saved: image/image_356.jpg\n",
      "✅ Saved: image/image_357.jpg\n",
      "✅ Saved: image/image_358.jpg\n",
      "✅ Saved: image/image_359.jpg\n",
      "✅ Saved: image/image_360.jpg\n",
      "✅ Saved: image/image_361.jpg\n",
      "✅ Saved: image/image_362.jpg\n",
      "✅ Saved: image/image_363.jpg\n",
      "✅ Saved: image/image_364.jpg\n",
      "✅ Saved: image/image_365.jpg\n",
      "✅ Saved: image/image_366.jpg\n",
      "✅ Saved: image/image_367.jpg\n",
      "✅ Saved: image/image_368.jpg\n",
      "✅ Saved: image/image_369.jpg\n",
      "✅ Saved: image/image_370.jpg\n",
      "✅ Saved: image/image_371.jpg\n",
      "✅ Saved: image/image_372.jpg\n",
      "✅ Saved: image/image_373.jpg\n",
      "✅ Saved: image/image_374.jpg\n",
      "✅ Saved: image/image_375.jpg\n",
      "✅ Saved: image/image_376.jpg\n",
      "✅ Saved: image/image_377.jpg\n",
      "✅ Saved: image/image_378.jpg\n",
      "✅ Saved: image/image_379.jpg\n",
      "✅ Saved: image/image_380.jpg\n",
      "✅ Saved: image/image_381.jpg\n",
      "✅ Saved: image/image_382.jpg\n",
      "✅ Saved: image/image_383.jpg\n",
      "✅ Saved: image/image_384.jpg\n",
      "✅ Saved: image/image_385.jpg\n",
      "✅ Saved: image/image_386.jpg\n",
      "✅ Saved: image/image_387.jpg\n",
      "✅ Saved: image/image_388.jpg\n",
      "✅ Saved: image/image_389.jpg\n",
      "✅ Saved: image/image_390.jpg\n",
      "✅ Saved: image/image_391.jpg\n",
      "✅ Saved: image/image_392.jpg\n",
      "✅ Saved: image/image_393.jpg\n",
      "✅ Saved: image/image_394.jpg\n",
      "✅ Saved: image/image_395.jpg\n",
      "✅ Saved: image/image_396.jpg\n",
      "✅ Saved: image/image_397.jpg\n",
      "✅ Saved: image/image_398.jpg\n",
      "✅ Saved: image/image_399.jpg\n",
      "✅ Updated DataFrame saved as 'updated_dataset.csv'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [11:18<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38e8778a06c495184887f3242388a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error pushing to Hugging Face Hub: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67f02457-5a998eb87aa4e42e295d5591;0c073dba-de67-4467-a6c3-203544cb3459)\n",
      "\n",
      "Invalid username or password.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>bbox</th>\n",
       "      <th>bbox_area</th>\n",
       "      <th>bbox_id</th>\n",
       "      <th>ori_category_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>file_name</th>\n",
       "      <th>is_rewrite</th>\n",
       "      <th>split</th>\n",
       "      <th>image</th>\n",
       "      <th>depth_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4381</td>\n",
       "      <td>A pair of tortillas containing a visible filli...</td>\n",
       "      <td>[300.25585936 239.42510986 183.92150884 133.25...</td>\n",
       "      <td>24508.697297</td>\n",
       "      <td>o365_24196196</td>\n",
       "      <td>o365_172</td>\n",
       "      <td>o365_489077</td>\n",
       "      <td>512</td>\n",
       "      <td>769</td>\n",
       "      <td>sampled_images/objects365_v1_00489077.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_0.jpg</td>\n",
       "      <td>Closest: a couple\\n----\\nMid Range: a bar\\n---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3385</td>\n",
       "      <td>The closed-top, white bucket-shaped item, posi...</td>\n",
       "      <td>[647.26123049 556.64379886  98.25061038 106.61...</td>\n",
       "      <td>10474.736003</td>\n",
       "      <td>o365_3289057</td>\n",
       "      <td>o365_49</td>\n",
       "      <td>o365_1040341</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_01040341.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_1.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: tents\\n----\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27674</td>\n",
       "      <td>The pink and white surfboard is curretnly ride...</td>\n",
       "      <td>[120.05 163.64 232.97  77.18]</td>\n",
       "      <td>17980.624600</td>\n",
       "      <td>coco_650459</td>\n",
       "      <td>refcoco_42</td>\n",
       "      <td>coco_449136</td>\n",
       "      <td>318</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000449136.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_2.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: a surfer\\n---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44294</td>\n",
       "      <td>The man wearing a cheerful Santa hat, characte...</td>\n",
       "      <td>[240.81  14.21 275.38 370.15]</td>\n",
       "      <td>101931.907000</td>\n",
       "      <td>coco_2165449</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_242583</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000242583.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_3.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: a hand\\n----\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29717</td>\n",
       "      <td>The pizza adorned with chunks of white topping...</td>\n",
       "      <td>[143.32 283.76 353.97 139.47]</td>\n",
       "      <td>49368.195900</td>\n",
       "      <td>coco_1074038</td>\n",
       "      <td>refcoco_59</td>\n",
       "      <td>coco_70415</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000070415.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_4.jpg</td>\n",
       "      <td>Closest: a pizza\\n----\\nMid Range: a person\\n-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>12011</td>\n",
       "      <td>The green purse hanging on the wall close to t...</td>\n",
       "      <td>[232.43835453 416.28540035  57.75231928  61.99...</td>\n",
       "      <td>3580.129160</td>\n",
       "      <td>o365_27275769</td>\n",
       "      <td>o365_14</td>\n",
       "      <td>o365_2053317</td>\n",
       "      <td>982</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_02053317.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_395.jpg</td>\n",
       "      <td>Closest: two motorcycles\\n----\\nMid Range: a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>36631</td>\n",
       "      <td>The man standing in the background, behind the...</td>\n",
       "      <td>[363.87 211.18 116.49 306.33]</td>\n",
       "      <td>35684.381700</td>\n",
       "      <td>coco_500529</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_242213</td>\n",
       "      <td>640</td>\n",
       "      <td>512</td>\n",
       "      <td>sampled_images/COCO_train2014_000000242213.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_396.jpg</td>\n",
       "      <td>Closest: a baseball pitcher\\n----\\nMid Range: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>18231</td>\n",
       "      <td>The skier clad in a red jacket is standing wit...</td>\n",
       "      <td>[474.02 105.55 130.5  316.65]</td>\n",
       "      <td>41322.825000</td>\n",
       "      <td>coco_512212</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_271641</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000271641.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_397.jpg</td>\n",
       "      <td>Closest: people\\n----\\nMid Range: a silhouette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>5846</td>\n",
       "      <td>A transparent drinking vessel containing a yel...</td>\n",
       "      <td>[284.04260255 188.13116452 100.05944822 199.01...</td>\n",
       "      <td>19913.766167</td>\n",
       "      <td>o365_9095135</td>\n",
       "      <td>o365_11</td>\n",
       "      <td>o365_476705</td>\n",
       "      <td>683</td>\n",
       "      <td>512</td>\n",
       "      <td>sampled_images/objects365_v1_00476705.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_398.jpg</td>\n",
       "      <td>Closest: candles\\n----\\nMid Range: a plate of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>16412</td>\n",
       "      <td>A tamed ruminant creature is nestled in the st...</td>\n",
       "      <td>[552.77685545 385.09814454 197.29370122 125.13...</td>\n",
       "      <td>24687.747138</td>\n",
       "      <td>o365_18759141</td>\n",
       "      <td>o365_100</td>\n",
       "      <td>o365_1693044</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_01693044.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_399.jpg</td>\n",
       "      <td>Closest: a car\\n----\\nMid Range: a car\\n----\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            caption  \\\n",
       "0     4381  A pair of tortillas containing a visible filli...   \n",
       "1     3385  The closed-top, white bucket-shaped item, posi...   \n",
       "2    27674  The pink and white surfboard is curretnly ride...   \n",
       "3    44294  The man wearing a cheerful Santa hat, characte...   \n",
       "4    29717  The pizza adorned with chunks of white topping...   \n",
       "..     ...                                                ...   \n",
       "395  12011  The green purse hanging on the wall close to t...   \n",
       "396  36631  The man standing in the background, behind the...   \n",
       "397  18231  The skier clad in a red jacket is standing wit...   \n",
       "398   5846  A transparent drinking vessel containing a yel...   \n",
       "399  16412  A tamed ruminant creature is nestled in the st...   \n",
       "\n",
       "                                                  bbox      bbox_area  \\\n",
       "0    [300.25585936 239.42510986 183.92150884 133.25...   24508.697297   \n",
       "1    [647.26123049 556.64379886  98.25061038 106.61...   10474.736003   \n",
       "2                        [120.05 163.64 232.97  77.18]   17980.624600   \n",
       "3                        [240.81  14.21 275.38 370.15]  101931.907000   \n",
       "4                        [143.32 283.76 353.97 139.47]   49368.195900   \n",
       "..                                                 ...            ...   \n",
       "395  [232.43835453 416.28540035  57.75231928  61.99...    3580.129160   \n",
       "396                      [363.87 211.18 116.49 306.33]   35684.381700   \n",
       "397                      [474.02 105.55 130.5  316.65]   41322.825000   \n",
       "398  [284.04260255 188.13116452 100.05944822 199.01...   19913.766167   \n",
       "399  [552.77685545 385.09814454 197.29370122 125.13...   24687.747138   \n",
       "\n",
       "           bbox_id ori_category_id      image_id  height  width  \\\n",
       "0    o365_24196196        o365_172   o365_489077     512    769   \n",
       "1     o365_3289057         o365_49  o365_1040341     768   1024   \n",
       "2      coco_650459      refcoco_42   coco_449136     318    640   \n",
       "3     coco_2165449       refcoco_1   coco_242583     427    640   \n",
       "4     coco_1074038      refcoco_59    coco_70415     429    640   \n",
       "..             ...             ...           ...     ...    ...   \n",
       "395  o365_27275769         o365_14  o365_2053317     982   1024   \n",
       "396    coco_500529       refcoco_1   coco_242213     640    512   \n",
       "397    coco_512212       refcoco_1   coco_271641     427    640   \n",
       "398   o365_9095135         o365_11   o365_476705     683    512   \n",
       "399  o365_18759141        o365_100  o365_1693044     768   1024   \n",
       "\n",
       "                                          file_name  is_rewrite split  \\\n",
       "0         sampled_images/objects365_v1_00489077.jpg        True   val   \n",
       "1         sampled_images/objects365_v2_01040341.jpg        True   val   \n",
       "2    sampled_images/COCO_train2014_000000449136.jpg        True   val   \n",
       "3    sampled_images/COCO_train2014_000000242583.jpg        True   val   \n",
       "4    sampled_images/COCO_train2014_000000070415.jpg       False   val   \n",
       "..                                              ...         ...   ...   \n",
       "395       sampled_images/objects365_v2_02053317.jpg        True   val   \n",
       "396  sampled_images/COCO_train2014_000000242213.jpg       False   val   \n",
       "397  sampled_images/COCO_train2014_000000271641.jpg       False   val   \n",
       "398       sampled_images/objects365_v1_00476705.jpg       False   val   \n",
       "399       sampled_images/objects365_v2_01693044.jpg        True   val   \n",
       "\n",
       "                   image                                      depth_caption  \n",
       "0      image/image_0.jpg  Closest: a couple\\n----\\nMid Range: a bar\\n---...  \n",
       "1      image/image_1.jpg  Closest: a man\\n----\\nMid Range: tents\\n----\\n...  \n",
       "2      image/image_2.jpg  Closest: a man\\n----\\nMid Range: a surfer\\n---...  \n",
       "3      image/image_3.jpg  Closest: a man\\n----\\nMid Range: a hand\\n----\\...  \n",
       "4      image/image_4.jpg  Closest: a pizza\\n----\\nMid Range: a person\\n-...  \n",
       "..                   ...                                                ...  \n",
       "395  image/image_395.jpg  Closest: two motorcycles\\n----\\nMid Range: a m...  \n",
       "396  image/image_396.jpg  Closest: a baseball pitcher\\n----\\nMid Range: ...  \n",
       "397  image/image_397.jpg  Closest: people\\n----\\nMid Range: a silhouette...  \n",
       "398  image/image_398.jpg  Closest: candles\\n----\\nMid Range: a plate of ...  \n",
       "399  image/image_399.jpg  Closest: a car\\n----\\nMid Range: a car\\n----\\n...  \n",
       "\n",
       "[400 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image as Image_PIL\n",
    "import shutil\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "hf_user = \"Rajarshi-Roy-research\"\n",
    "# dataset_name = \"Flickr30k_Grounding_Som\"\n",
    "# commit_message = \"Updating dataset with image paths\"\n",
    "# img_cols = [\"image\",\"wbox_image\"]\n",
    "# image_to_process = \"image\"\n",
    "\n",
    "dataset_name = \"refcocog-eval-depth\"\n",
    "commit_message = \"Updating dataset with images and depth caption\"\n",
    "img_cols = [\"image\"]\n",
    "image_to_process = \"image\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_image_from_bytes(image_data, idx, image_folder):\n",
    "    \"\"\"Saves an image from byte data and returns the saved file path.\"\"\"\n",
    "    try:\n",
    "        image_bytes = image_data.get(\"bytes\", None)\n",
    "        if image_bytes:\n",
    "            image = Image_PIL.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(image_folder, f\"image_{idx}.jpg\")\n",
    "            image.save(image_path, format=\"JPEG\")\n",
    "            print(f\"✅ Saved: {image_path}\")\n",
    "            return image_path\n",
    "        else:\n",
    "            print(f\"⚠️ Skipped row {idx}: No bytes found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving image at row {idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_image_from_df(df, depth_kosmos_captioner, image_folder):\n",
    "    \"\"\"Processes the DataFrame, adding depth captions and saving progress.\"\"\"\n",
    "    if \"depth_caption\" not in df.columns:\n",
    "        df[\"depth_caption\"] = None\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if row[\"depth_caption\"] is None:\n",
    "            try:\n",
    "                if isinstance(row[\"image\"], dict): #Handle case where image is still a dict\n",
    "                    image_path = save_image_from_bytes(row[\"image\"], index, image_folder)\n",
    "                    if image_path:\n",
    "                        image = Image_PIL.open(image_path)\n",
    "                        full_caption_string = depth_kosmos_captioner.get_caption_with_depth(image)\n",
    "                        df.loc[index, \"depth_caption\"] = full_caption_string\n",
    "                elif isinstance(row[\"image\"], str): #Handle case where image is a file path\n",
    "                    image = Image_PIL.open(row[\"image\"])\n",
    "                    full_caption_string = depth_kosmos_captioner.get_caption_with_depth(image)\n",
    "                    df.loc[index, \"depth_caption\"] = full_caption_string\n",
    "                else:\n",
    "                    print(f\"⚠️ Unexpected image type at index {index}: {type(row['image'])}\")\n",
    "                    df.loc[index, \"depth_caption\"] = \"Error: Unexpected image type\"\n",
    "\n",
    "                df.to_parquet(\"df_with_captions.parquet\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image at index {index}: {e}\")\n",
    "                df.loc[index, \"depth_caption\"] = \"Error\"\n",
    "                df.to_parquet(\"df_with_captions.parquet\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def push_df_to_hugging_face(df, img_cols, hf_user, dataset_name, commit_message, hf_token, private=False):\n",
    "    \"\"\"Pushes the DataFrame to the Hugging Face Hub.\"\"\"\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    for img_col in img_cols:\n",
    "        hf_dataset = hf_dataset.cast_column(img_col, Image())\n",
    "\n",
    "    try:\n",
    "        hf_dataset.push_to_hub(\n",
    "            repo_id=f\"{hf_user}/{dataset_name}\",\n",
    "            commit_message=commit_message,\n",
    "            token=hf_token,\n",
    "            private=private,\n",
    "        )\n",
    "        print(f\"✅ Dataset successfully uploaded: {hf_user}/{dataset_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pushing to Hugging Face Hub: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "# Clean up existing image folder\n",
    "for img_col in img_cols:\n",
    "    if os.path.exists(img_col):\n",
    "        shutil.rmtree(img_col)\n",
    "    os.makedirs(img_col)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(f\"{hf_user}/{dataset_name}\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "for img_col in img_cols:\n",
    "    # Save images and update DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if isinstance(row[img_col], dict):\n",
    "            df.at[i, img_col] = save_image_from_bytes(row[img_col], i, img_col)\n",
    "\n",
    "df.to_csv(\"updated_dataset.csv\", index=False)\n",
    "print(\"✅ Updated DataFrame saved as 'updated_dataset.csv'!\")\n",
    "\n",
    "#Process and add captions\n",
    "try:\n",
    "    df = pd.read_csv(\"updated_dataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "df = process_image_from_df(df, depth_kosmos_captioner, image_to_process)\n",
    "\n",
    "# Authenticate and push to Hugging Face\n",
    "\n",
    "login(token=hf_token)\n",
    "push_df_to_hugging_face(df, img_cols, hf_user, dataset_name, commit_message, hf_token, private=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbaf03c352d4cb6ad1d9de744496187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d458415604eac8cccd335bb2e0fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86371a29e242488f91e2aec78ed5b7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset successfully uploaded: Rajarshi-Roy-research/refcocog-eval-depth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login(token=hf_token)\n",
    "push_df_to_hugging_face(df, img_cols, hf_user, dataset_name, commit_message, hf_token, private=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
