{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /root/Depth-SoM/Depth-Anything-V2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Verify the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Change to the desired directory\n",
    "os.chdir(f\"{current_dir}/Depth-Anything-V2\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['000000007601'], 'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1205x800 at 0x78AF477C9720>], 'obj_text': [['A black cow that is only half visible standing close to a fully visible cow.', 'The far right black cow.; cow on the far right who is barely visible; cow butt 3:00; cow barely showing; cow far right; far right cow butt', 'A cow standing in high grass wearing an ear tag with the number 342.; Cow with a number 312 on a tag on its ear.; yellow tag in ear; cow with tag; Cow with yellow tag; middle cow; big cow yellow tag; center cow']], 'ref_ids': [[3, 1, 2]], 'json_data': [[{'ref_id': 3, 'text': ['A black cow that is only half visible standing close to a fully visible cow.']}, {'ref_id': 1, 'text': ['The far right black cow.', 'cow on the far right who is barely visible', 'cow butt 3:00', 'cow barely showing', 'cow far right', 'far right cow butt']}, {'ref_id': 2, 'text': ['A cow standing in high grass wearing an ear tag with the number 342.', 'Cow with a number 312 on a tag on its ear.', 'yellow tag in ear', 'cow with tag', 'Cow with yellow tag', 'middle cow', 'big cow yellow tag', 'center cow']}]], 'depth_caption': ['Closest: a cow\\n----\\nMid Range: a power line\\n----\\nFarthest: an elephant\\n----\\n']}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc9dfc116934ee8862eefe5b5ab70f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_0.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_1.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_2.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_3.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_4.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_5.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_6.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_7.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_8.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_9.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_10.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_11.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_12.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_13.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_14.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_15.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_16.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_17.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_18.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_19.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_20.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_21.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_22.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_23.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_24.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_25.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_26.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_27.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_28.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_29.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_30.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_31.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_32.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_33.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_34.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_35.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_36.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_37.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_38.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_39.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_40.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_41.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_42.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_43.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_44.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_45.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_46.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_47.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_48.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_49.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_50.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_51.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_52.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_53.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_54.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_55.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_56.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_57.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_58.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_59.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_60.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_61.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_62.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_63.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_64.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_65.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_66.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_67.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_68.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_69.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_70.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_71.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_72.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_73.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_74.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_75.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_76.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_77.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_78.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_79.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_80.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_81.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_82.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_83.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_84.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_85.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_86.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_87.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_88.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_89.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_90.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_91.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_92.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_93.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_94.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_95.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_96.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_97.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_98.jpg\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "✅ Saved: image/image_99.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': ['000000007601'],\n",
       " 'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1205x800>],\n",
       " 'obj_text': [['A black cow that is only half visible standing close to a fully visible cow.',\n",
       "   'The far right black cow.; cow on the far right who is barely visible; cow butt 3:00; cow barely showing; cow far right; far right cow butt',\n",
       "   'A cow standing in high grass wearing an ear tag with the number 342.; Cow with a number 312 on a tag on its ear.; yellow tag in ear; cow with tag; Cow with yellow tag; middle cow; big cow yellow tag; center cow']],\n",
       " 'ref_ids': [[3, 1, 2]],\n",
       " 'json_data': [[{'ref_id': 3,\n",
       "    'text': ['A black cow that is only half visible standing close to a fully visible cow.']},\n",
       "   {'ref_id': 1,\n",
       "    'text': ['The far right black cow.',\n",
       "     'cow on the far right who is barely visible',\n",
       "     'cow butt 3:00',\n",
       "     'cow barely showing',\n",
       "     'cow far right',\n",
       "     'far right cow butt']},\n",
       "   {'ref_id': 2,\n",
       "    'text': ['A cow standing in high grass wearing an ear tag with the number 342.',\n",
       "     'Cow with a number 312 on a tag on its ear.',\n",
       "     'yellow tag in ear',\n",
       "     'cow with tag',\n",
       "     'Cow with yellow tag',\n",
       "     'middle cow',\n",
       "     'big cow yellow tag',\n",
       "     'center cow']}]],\n",
       " 'depth_caption': ['Closest: a cow\\n----\\nMid Range: a power line\\n----\\nFarthest: an elephant\\n----\\n']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from PIL import Image as Image_PIL\n",
    "import shutil\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)\n",
    "\n",
    "hf_user = \"Rajarshi-Roy-research\"\n",
    "# dataset_name = \"ADE20K_OVSEG_Som\"\n",
    "dataset_name = \"Som_bench_refcocog_refseg\"\n",
    "commit_message = \"Updating dataset with VLM\"\n",
    "img_cols = [\"image\"]\n",
    "image_to_process = \"image\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_image_from_bytes(image_data, idx, image_folder):\n",
    "    \n",
    "    if isinstance(image_data,list):\n",
    "        image_data = image_data[0]\n",
    "        \n",
    "    \"\"\"Saves an image from byte data or PIL Image and returns the saved file path.\"\"\"\n",
    "    print(type(image_data))\n",
    "    try:\n",
    "        if isinstance(image_data, Image_PIL.Image):\n",
    "            image = image_data\n",
    "        elif isinstance(image_data, bytes):\n",
    "            image = Image_PIL.open(io.BytesIO(image_data))\n",
    "        else:\n",
    "            print(f\"Unsupported image data type at row {idx}: {type(image_data)}\")\n",
    "            return None\n",
    "\n",
    "        image_path = os.path.join(image_folder, f\"image_{idx}.jpg\")\n",
    "        image.save(image_path, format=\"JPEG\")\n",
    "        print(f\"✅ Saved: {image_path}\")\n",
    "        return image_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving image at row {idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def push_to_hugging_face(ds, img_cols, hf_user, dataset_name, commit_message, hf_token, private=False):\n",
    "    \"\"\"Pushes the dataset to the Hugging Face Hub.\"\"\"\n",
    "    for img_col in img_cols:\n",
    "        ds = ds.cast_column(img_col, Image())\n",
    "\n",
    "    try:\n",
    "        ds.push_to_hub(\n",
    "            repo_id=f\"{hf_user}/{dataset_name}\",\n",
    "            commit_message=commit_message,\n",
    "            token=hf_token,\n",
    "            private=private,\n",
    "        )\n",
    "        print(f\"✅ Dataset successfully uploaded: {hf_user}/{dataset_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pushing to Hugging Face Hub: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "# Clean up existing image folder\n",
    "for img_col in img_cols:\n",
    "    if os.path.exists(img_col):\n",
    "        shutil.rmtree(img_col)\n",
    "    os.makedirs(img_col)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(f\"{hf_user}/{dataset_name}\",revision=\"a8196e2e83ea9c13ae9d3ed421c0edc27cc4a420\")\n",
    "# ds = load_dataset(f\"{hf_user}/{dataset_name}\")\n",
    "print(ds[\"train\"][:1])\n",
    "\n",
    "#Process images using the dataset object directly.\n",
    "for img_col in img_cols:\n",
    "    ds = ds.map(\n",
    "        lambda example, idx: {img_col: save_image_from_bytes(example[img_col], idx, img_col)},\n",
    "        with_indices=True,\n",
    "        num_proc=1 # Adjust as needed for performance\n",
    "    )\n",
    "    \n",
    "\n",
    "# push_to_hugging_face(ds, img_cols, hf_user, dataset_name, commit_message, hf_token)\n",
    "\n",
    "#Process and add captions (This part remains unchanged as it doesn't directly interact with image data)\n",
    "try:\n",
    "    #This section is not needed anymore since we are working directly with the dataset.\n",
    "    #df = pd.read_csv(\"updated_dataset.csv\") \n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "ds[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:39:16.665589: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-10 13:39:16.675712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741613956.690200    1217 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741613956.694533    1217 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 13:39:16.708823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "import requests\n",
    "from PIL import Image as Image_PIL\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor and model, then move the model to the chosen device\n",
    "processor = Pix2StructProcessor.from_pretrained('google/deplot')\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 2]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_list_from_string(text):\n",
    "    \"\"\"\n",
    "    Extracts a list from a string by finding the first occurrence of square brackets.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input string containing a Python list.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted list if found, otherwise an empty list.\n",
    "        \n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\[.*?\\]\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return ast.literal_eval(match.group(0))  # Safely parse the list\n",
    "        except (SyntaxError, ValueError):\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Example usage\n",
    "text = \"```python\\n[3, 1, 2]\\n```\"\n",
    "print(extract_list_from_string(text))  # Output: [3, 1, 2]\n",
    "print(type(extract_list_from_string(text)))  # Output: [3, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import base64\n",
    "import ast\n",
    "\n",
    "def text_to_list(text):\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(obj_list):\n",
    "  prompt=f\"\"\"I have labeled a bright numeric ID at the center for each visual object in the image. \\nPlease tell me the IDs for:\"\"\"\n",
    "\n",
    "  for i in obj_list:\n",
    "    prompt += f\"\\n-{i}\\n\"\n",
    "\n",
    "  prompt+= \"\\nPlease, properly give answer in the form of just a python list format accoriding to the order of objects asked\\nAnswer Example:`[1,2,4..]`\"\n",
    "\n",
    "  # print(prompt)\n",
    "  return prompt\n",
    "\n",
    "def get_response(prompt,img_path):\n",
    "  \n",
    "  if isinstance(img_path, Image_PIL.Image):\n",
    "    image = img_path\n",
    "  \n",
    "  elif isinstance(img_path, str):\n",
    "    image = Image_PIL.open(img_path)\n",
    "  print(prompt)\n",
    "  \n",
    "  # Prepare inputs and move them to the device\n",
    "  inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "  inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "  # Generate predictions on the GPU\n",
    "  predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "  # Decode and print the output\n",
    "  response = processor.decode(predictions[0], skip_special_tokens=True)\n",
    "  \n",
    "  print(\"response\",response)\n",
    "  print(extract_list_from_string(response))\n",
    "\n",
    "\n",
    "def get_prediction(index, ds):\n",
    "\n",
    "  img_path = ds[index]['image']\n",
    "  obj_list = ds[index]['obj_text']\n",
    "\n",
    "  prompt = get_prompt(obj_list)\n",
    "  #print(prompt)\n",
    "  return get_response(prompt,img_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, If both images and text are provided, image_processor is not a VQA processor, and `add_special_tokens` is unset, the default value of `add_special_tokens` will be changed to `False` when calling the tokenizer. To test the new behavior, set `legacy=False`as a processor call argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have labeled a bright numeric ID at the center for each visual object in the image. \n",
      "Please tell me the IDs for:\n",
      "-little girl in light green shirt; Little Girl in a green dress waiting for the cake to be cut.; green; green dress; green shirt; Girl in green; girl in green; girl in green\n",
      "\n",
      "-the blonde girls head; blonde hair with small green hairtie; blonde hair; blonde hair; blonde girl; blonde hair girl on left; hair in left corner; blonde head\n",
      "\n",
      "-A knife cutting a cake.; Person wearing jacket cutting the cake.; standing man; person cutting cake; cutting the cake; man cutting cake\n",
      "\n",
      "-girl with glasses; glasses; girl with glasses; right kid; right gal!; right kid\n",
      "\n",
      "Please, properly give answer in the form of just a python list format accoriding to the order of objects asked\n",
      "Answer Example:`[1,2,4..]`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response TITLE |  <0x0A>  | 1 <0x0A> 1 | 1 <0x0A> 2 | 2 <0x0A> 3 | 3 <0x0A> 4 | 4 <0x0A> 5 | 5 <0x0A> 6 | 6 <0x0A> 7 | 7 <0x0A> 8 | 10 <0x0A> 9 | 9 <0x0A> 10 | 11 <0x0A> 11 | 7 <0x0A> 12 | 13 <0x0A> 13 | 14 <0x0A> 14 | 11 <0x0A> 15 | 10 <0x0A> 16 | 10 <0x0A> 17 | 10 <0x0A> 18 | 10 <0x0A> 19 | 10 <0x0A> 20 | 10 <0x0A> 21 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 21 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 10 <0x0A> 20 | 1\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "list_ =  get_prediction(1,ds[\"train\"])\n",
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1f899ddf08466e983c6fee0a225dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(0)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "What are these? ASSISTANT: These are two cats lying on a pink couch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "What are these? ASSISTANT: These are two cats lying on a pink couch.\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode(output[0][2:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
