{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image as Image_PIL\n",
    "import shutil\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "hf_user = \"Rajarshi-Roy-research\"\n",
    "# dataset_name = \"Flickr30k_Grounding_Som\"\n",
    "# commit_message = \"Updating dataset with image paths\"\n",
    "# img_cols = [\"image\",\"wbox_image\"]\n",
    "# image_to_process = \"image\"\n",
    "\n",
    "dataset_name = \"Som_bench_refcocog_refseg\"\n",
    "commit_message = \"Updating dataset with VLM\"\n",
    "img_cols = [\"image\"]\n",
    "image_to_process = \"image\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_image_from_bytes(image_data, idx, image_folder):\n",
    "    \"\"\"Saves an image from byte data and returns the saved file path.\"\"\"\n",
    "    try:\n",
    "        image_bytes = image_data.get(\"bytes\", None)\n",
    "        if image_bytes:\n",
    "            image = Image_PIL.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(image_folder, f\"image_{idx}.jpg\")\n",
    "            image.save(image_path, format=\"JPEG\")\n",
    "            print(f\"✅ Saved: {image_path}\")\n",
    "            return image_path\n",
    "        else:\n",
    "            print(f\"⚠️ Skipped row {idx}: No bytes found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving image at row {idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def push_df_to_hugging_face(df, img_cols, hf_user, dataset_name, commit_message, hf_token, private=False):\n",
    "    \"\"\"Pushes the DataFrame to the Hugging Face Hub.\"\"\"\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    for img_col in img_cols:\n",
    "        hf_dataset = hf_dataset.cast_column(img_col, Image())\n",
    "\n",
    "    try:\n",
    "        hf_dataset.push_to_hub(\n",
    "            repo_id=f\"{hf_user}/{dataset_name}\",\n",
    "            commit_message=commit_message,\n",
    "            token=hf_token,\n",
    "            private=private,\n",
    "        )\n",
    "        print(f\"✅ Dataset successfully uploaded: {hf_user}/{dataset_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pushing to Hugging Face Hub: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "# Clean up existing image folder\n",
    "for img_col in img_cols:\n",
    "    if os.path.exists(img_col):\n",
    "        shutil.rmtree(img_col)\n",
    "    os.makedirs(img_col)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(f\"{hf_user}/{dataset_name}\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "for img_col in img_cols:\n",
    "    # Save images and update DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if isinstance(row[img_col], dict):\n",
    "            df.at[i, img_col] = save_image_from_bytes(row[img_col], i, img_col)\n",
    "\n",
    "df.to_csv(\"updated_dataset.csv\", index=False)\n",
    "print(\"✅ Updated DataFrame saved as 'updated_dataset.csv'!\")\n",
    "\n",
    "#Process and add captions\n",
    "try:\n",
    "    df = pd.read_csv(\"updated_dataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c90930748d40709d0debedd83e741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error saving image at row ['000000007601']: 'list' object has no attribute 'get'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'list'>, <class 'NoneType'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'tensorflow.python.framework.tensor.Tensor'>, <class 'torch.Tensor'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m save_image_from_bytes(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], image_to_process)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[0;32m---> 16\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Adjust batch_size as needed\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ds\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Updated DataFrame saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/dataset_dict.py:901\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 901\u001b[0m     {\n\u001b[1;32m    902\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    903\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    904\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    905\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    906\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    907\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    908\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    909\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    910\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    911\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    912\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    913\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    914\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    915\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    916\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    917\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    918\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    919\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    920\u001b[0m         )\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    922\u001b[0m     }\n\u001b[1;32m    923\u001b[0m )\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/dataset_dict.py:902\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    901\u001b[0m     {\n\u001b[0;32m--> 902\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    922\u001b[0m     }\n\u001b[1;32m    923\u001b[0m )\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3519\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3518\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3519\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3520\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[1;32m   3521\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3393\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m-> 3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3357\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.prepare_outputs\u001b[0;34m(pa_inputs, inputs, processed_inputs)\u001b[0m\n\u001b[1;32m   3355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3356\u001b[0m     returned_lazy_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3357\u001b[0m \u001b[43mvalidate_function_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard\u001b[38;5;241m.\u001b[39m_format_type \u001b[38;5;129;01mor\u001b[39;00m input_columns:\n\u001b[1;32m   3359\u001b[0m     \u001b[38;5;66;03m# TODO(QL, MS): ideally the behavior should be the same even if the dataset is formatted (may require major release)\u001b[39;00m\n\u001b[1;32m   3360\u001b[0m     inputs_to_merge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(pa_inputs\u001b[38;5;241m.\u001b[39mcolumn_names, pa_inputs\u001b[38;5;241m.\u001b[39mitercolumns()))\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3322\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[0;34m(processed_inputs)\u001b[0m\n\u001b[1;32m   3318\u001b[0m all_dict_values_are_lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   3319\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   3320\u001b[0m )\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_dict_values_are_lists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 3322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mprocessed_inputs\u001b[38;5;241m.\u001b[39mvalues()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_batch_return_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3324\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'list'>, <class 'NoneType'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'tensorflow.python.framework.tensor.Tensor'>, <class 'torch.Tensor'>)`."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Script ---\n",
    "# Clean up existing image folder\n",
    "for img_col in img_cols:\n",
    "    if os.path.exists(img_col):\n",
    "        shutil.rmtree(img_col)\n",
    "    os.makedirs(img_col)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(f\"{hf_user}/{dataset_name}\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "for img_col in img_cols:\n",
    "    # Save images and update DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if isinstance(row[img_col], dict):\n",
    "            df.at[i, img_col] = save_image_from_bytes(row[img_col], i, img_col)\n",
    "\n",
    "df.to_csv(\"updated_dataset.csv\", index=False)\n",
    "print(\"✅ Updated DataFrame saved as 'updated_dataset.csv'!\")\n",
    "\n",
    "#Process and add captions\n",
    "try:\n",
    "    df = pd.read_csv(\"updated_dataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 12:19:58.474292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-10 12:19:58.484240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741609198.498548    1371 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741609198.502794    1371 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 12:19:58.517699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3334071a196e4f189a75b1c0f03d1d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4cfea4abd344d2a8565b95d9653c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7178483fd45b49d1a2e94925c06f61c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c391862fc23455cbb43267036484686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7ae90485074287bb81be6099bca770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a7542074f49e88e21903c5270fdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe3737ccf7e46de989bbdce31a8f7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "import requests\n",
    "from PIL import Image as Image_PIL\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor and model, then move the model to the chosen device\n",
    "processor = Pix2StructProcessor.from_pretrained('google/deplot')\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>obj_text</th>\n",
       "      <th>ref_ids</th>\n",
       "      <th>json_data</th>\n",
       "      <th>depth_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7601</td>\n",
       "      <td>image/image_0.jpg</td>\n",
       "      <td>['A black cow that is only half visible standi...</td>\n",
       "      <td>[3 1 2]</td>\n",
       "      <td>[{'ref_id': 3, 'text': array(['A black cow tha...</td>\n",
       "      <td>Closest: a cow\\n----\\nMid Range: a power line\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31187</td>\n",
       "      <td>image/image_1.jpg</td>\n",
       "      <td>['little girl in light green shirt; Little Gir...</td>\n",
       "      <td>[2 1 3 4]</td>\n",
       "      <td>[{'ref_id': 2, 'text': array(['little girl in ...</td>\n",
       "      <td>Closest: a birthday cake\\n----\\nMid Range: a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66669</td>\n",
       "      <td>image/image_2.jpg</td>\n",
       "      <td>['an orange cat stuffed animal; The teddy bear...</td>\n",
       "      <td>[4 5]</td>\n",
       "      <td>[{'ref_id': 4, 'text': array(['an orange cat s...</td>\n",
       "      <td>Closest: a baby\\n----\\nMid Range: a cow\\n----\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17997</td>\n",
       "      <td>image/image_3.jpg</td>\n",
       "      <td>['The giraffe standing in the middle.; A giraf...</td>\n",
       "      <td>[3 2]</td>\n",
       "      <td>[{'ref_id': 3, 'text': array(['The giraffe sta...</td>\n",
       "      <td>Closest: three giraffes\\n----\\nMid Range: a pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32707</td>\n",
       "      <td>image/image_4.jpg</td>\n",
       "      <td>['skier in orange coat in the right hand pictu...</td>\n",
       "      <td>[2 1]</td>\n",
       "      <td>[{'ref_id': 2, 'text': array(['skier in orange...</td>\n",
       "      <td>Closest: two people\\n----\\nMid Range: the Eart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>37862</td>\n",
       "      <td>image/image_95.jpg</td>\n",
       "      <td>['the little dog with black fur; smallest; bot...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[{'ref_id': 2, 'text': array(['the little dog ...</td>\n",
       "      <td>Closest: three cats\\n----\\nMid Range: a guitar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11774</td>\n",
       "      <td>image/image_96.jpg</td>\n",
       "      <td>['man holding two plates; Short guy with smore...</td>\n",
       "      <td>[2 1]</td>\n",
       "      <td>[{'ref_id': 2, 'text': array(['man holding two...</td>\n",
       "      <td>Closest: two men\\n----\\nMid Range: a couple\\n-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7852</td>\n",
       "      <td>image/image_97.jpg</td>\n",
       "      <td>['The old school fire truck with a blue engine...</td>\n",
       "      <td>[1 2]</td>\n",
       "      <td>[{'ref_id': 1, 'text': array(['The old school ...</td>\n",
       "      <td>Closest: a red tractor\\n----\\nMid Range: a bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2411</td>\n",
       "      <td>image/image_98.jpg</td>\n",
       "      <td>['The white piece of cake; The slice of cake o...</td>\n",
       "      <td>[5 4]</td>\n",
       "      <td>[{'ref_id': 5, 'text': array(['The white piece...</td>\n",
       "      <td>Closest: a broken plate\\n----\\nMid Range: a pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>34739</td>\n",
       "      <td>image/image_99.jpg</td>\n",
       "      <td>['A smiling bear covered in green moss.; Rocks...</td>\n",
       "      <td>[2 1]</td>\n",
       "      <td>[{'ref_id': 2, 'text': array(['A smiling bear ...</td>\n",
       "      <td>Closest: a teddy bear\\n----\\nMid Range: a ston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id               image  \\\n",
       "0    7601   image/image_0.jpg   \n",
       "1   31187   image/image_1.jpg   \n",
       "2   66669   image/image_2.jpg   \n",
       "3   17997   image/image_3.jpg   \n",
       "4   32707   image/image_4.jpg   \n",
       "..    ...                 ...   \n",
       "95  37862  image/image_95.jpg   \n",
       "96  11774  image/image_96.jpg   \n",
       "97   7852  image/image_97.jpg   \n",
       "98   2411  image/image_98.jpg   \n",
       "99  34739  image/image_99.jpg   \n",
       "\n",
       "                                             obj_text    ref_ids  \\\n",
       "0   ['A black cow that is only half visible standi...    [3 1 2]   \n",
       "1   ['little girl in light green shirt; Little Gir...  [2 1 3 4]   \n",
       "2   ['an orange cat stuffed animal; The teddy bear...      [4 5]   \n",
       "3   ['The giraffe standing in the middle.; A giraf...      [3 2]   \n",
       "4   ['skier in orange coat in the right hand pictu...      [2 1]   \n",
       "..                                                ...        ...   \n",
       "95  ['the little dog with black fur; smallest; bot...        [2]   \n",
       "96  ['man holding two plates; Short guy with smore...      [2 1]   \n",
       "97  ['The old school fire truck with a blue engine...      [1 2]   \n",
       "98  ['The white piece of cake; The slice of cake o...      [5 4]   \n",
       "99  ['A smiling bear covered in green moss.; Rocks...      [2 1]   \n",
       "\n",
       "                                            json_data  \\\n",
       "0   [{'ref_id': 3, 'text': array(['A black cow tha...   \n",
       "1   [{'ref_id': 2, 'text': array(['little girl in ...   \n",
       "2   [{'ref_id': 4, 'text': array(['an orange cat s...   \n",
       "3   [{'ref_id': 3, 'text': array(['The giraffe sta...   \n",
       "4   [{'ref_id': 2, 'text': array(['skier in orange...   \n",
       "..                                                ...   \n",
       "95  [{'ref_id': 2, 'text': array(['the little dog ...   \n",
       "96  [{'ref_id': 2, 'text': array(['man holding two...   \n",
       "97  [{'ref_id': 1, 'text': array(['The old school ...   \n",
       "98  [{'ref_id': 5, 'text': array(['The white piece...   \n",
       "99  [{'ref_id': 2, 'text': array(['A smiling bear ...   \n",
       "\n",
       "                                        depth_caption  \n",
       "0   Closest: a cow\\n----\\nMid Range: a power line\\...  \n",
       "1   Closest: a birthday cake\\n----\\nMid Range: a b...  \n",
       "2   Closest: a baby\\n----\\nMid Range: a cow\\n----\\...  \n",
       "3   Closest: three giraffes\\n----\\nMid Range: a pe...  \n",
       "4   Closest: two people\\n----\\nMid Range: the Eart...  \n",
       "..                                                ...  \n",
       "95  Closest: three cats\\n----\\nMid Range: a guitar...  \n",
       "96  Closest: two men\\n----\\nMid Range: a couple\\n-...  \n",
       "97  Closest: a red tractor\\n----\\nMid Range: a bla...  \n",
       "98  Closest: a broken plate\\n----\\nMid Range: a pa...  \n",
       "99  Closest: a teddy bear\\n----\\nMid Range: a ston...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 2]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_list_from_string(text):\n",
    "    \"\"\"\n",
    "    Extracts a list from a string by finding the first occurrence of square brackets.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input string containing a Python list.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted list if found, otherwise an empty list.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\[.*?\\]\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return ast.literal_eval(match.group(0))  # Safely parse the list\n",
    "        except (SyntaxError, ValueError):\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Example usage\n",
    "text = \"```python\\n[3, 1, 2]\\n```\"\n",
    "print(extract_list_from_string(text))  # Output: [3, 1, 2]\n",
    "print(type(extract_list_from_string(text)))  # Output: [3, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://t4.ftcdn.net/jpg/01/62/69/25/360_F_162692511_SidIKVCDnt5UKHPNqpCb2MSKvfBlx1lG.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image\n",
    "\n",
    "# Prepare inputs and move them to the device\n",
    "inputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\n",
    "inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "# Generate predictions on the GPU\n",
    "predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# Decode and print the output\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import base64\n",
    "import ast\n",
    "\n",
    "def text_to_list(text):\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(obj_list):\n",
    "  prompt=f\"\"\"I have labeled a bright numeric ID at the center for each visual object in the image. \\nPlease tell me the IDs for:\"\"\"\n",
    "\n",
    "  for i in obj_list:\n",
    "    prompt += f\"\\n-{i}\\n\"\n",
    "\n",
    "  prompt+= \"\\nPlease, properly give answer in the form of just a python list format accoriding to the order of objects asked\\nAnswer Example:`[1,2,4..]`\"\n",
    "\n",
    "  # print(prompt)\n",
    "  return prompt\n",
    "\n",
    "def get_response(prompt,img_path):\n",
    "  \n",
    "  image = Image_PIL.open(img_path)\n",
    "  print(prompt)\n",
    "  \n",
    "  # Prepare inputs and move them to the device\n",
    "  inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "  inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "  # Generate predictions on the GPU\n",
    "  predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "  # Decode and print the output\n",
    "  response = processor.decode(predictions[0], skip_special_tokens=True)\n",
    "  \n",
    "  print(\"response\",response)\n",
    "  print(extract_list_from_string(response))\n",
    "\n",
    "\n",
    "def get_prediction(index):\n",
    "\n",
    "  img_path = df.loc[index,\"image\"]\n",
    "  obj_list = text_to_list(df.loc[index,\"obj_text\"])\n",
    "\n",
    "  prompt = get_prompt(obj_list)\n",
    "  print(prompt)\n",
    "  return obj_list\n",
    "\n",
    "  # return get_response(prompt,img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have labeled a bright numeric ID at the center for each visual object in the image. \n",
      "Please tell me the IDs for:\n",
      "-little girl in light green shirt; Little Girl in a green dress waiting for the cake to be cut.; green; green dress; green shirt; Girl in green; girl in green; girl in greenthe blonde girls head; blonde hair with small green hairtie; blonde hair; blonde hair; blonde girl; blonde hair girl on left; hair in left corner; blonde headA knife cutting a cake.; Person wearing jacket cutting the cake.; standing man; person cutting cake; cutting the cake; man cutting cakegirl with glasses; glasses; girl with glasses; right kid; right gal!; right kid\n",
      "\n",
      "Please, properly give answer in the form of just a python list format accoriding to the order of objects asked\n",
      "Answer Example:`[1,2,4..]`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['little girl in light green shirt; Little Girl in a green dress waiting for the cake to be cut.; green; green dress; green shirt; Girl in green; girl in green; girl in greenthe blonde girls head; blonde hair with small green hairtie; blonde hair; blonde hair; blonde girl; blonde hair girl on left; hair in left corner; blonde headA knife cutting a cake.; Person wearing jacket cutting the cake.; standing man; person cutting cake; cutting the cake; man cutting cakegirl with glasses; glasses; girl with glasses; right kid; right gal!; right kid']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ =  get_prediction(1)\n",
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
