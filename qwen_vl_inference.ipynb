{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install qwen-vl-utils\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from flash-attn) (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (4.13.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187815087 sha256=ffe17686fa1a0f288de9eae7c32af209d32a27b037ef28614f042b377af5b15a\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fed6d83f4c439c8720bd8a0f9a0a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3facd44c4b41f4a5d87a8ec96edc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcfb503ff2e4fa69275add8e35d4aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61568cf45a4a529a8a22ea712613b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4ecffb7dbd45cea007129eeee60dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23018c964bea4b80a1400a6667bf66b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89e7ca6d3744567ac89844ec6d71447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06e966538ae4105b708f079a74d16b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "E: Unable to locate package fonts-noto-cjk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get Noto JP font to display janapese characters\n",
    "!apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n",
    "\n",
    "#!apt-get install fonts-source-han-sans-jp # For Source Han Sans (Japanese)\n",
    "\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def decode_xml_points(text):\n",
    "    try:\n",
    "        root = ET.fromstring(text)\n",
    "        num_points = (len(root.attrib) - 1) // 2\n",
    "        points = []\n",
    "        for i in range(num_points):\n",
    "            x = root.attrib.get(f'x{i+1}')\n",
    "            y = root.attrib.get(f'y{i+1}')\n",
    "            points.append([x, y])\n",
    "        alt = root.attrib.get('alt')\n",
    "        phrase = root.text.strip() if root.text else None\n",
    "        return {\n",
    "            \"points\": points,\n",
    "            \"alt\": alt,\n",
    "            \"phrase\": phrase\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def plot_bounding_boxes(im, bounding_boxes, input_width, input_height):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    'pink',\n",
    "    'purple',\n",
    "    'brown',\n",
    "    'gray',\n",
    "    'beige',\n",
    "    'turquoise',\n",
    "    'cyan',\n",
    "    'magenta',\n",
    "    'lime',\n",
    "    'navy',\n",
    "    'maroon',\n",
    "    'teal',\n",
    "    'olive',\n",
    "    'coral',\n",
    "    'lavender',\n",
    "    'violet',\n",
    "    'gold',\n",
    "    'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Parsing out the markdown fencing\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "\n",
    "    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
    "\n",
    "    try:\n",
    "      json_output = ast.literal_eval(bounding_boxes)\n",
    "    except Exception as e:\n",
    "      end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "      truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "      json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "    # Iterate over the bounding boxes\n",
    "    for i, bounding_box in enumerate(json_output):\n",
    "      # Select a color from the list\n",
    "      color = colors[i % len(colors)]\n",
    "\n",
    "      # Convert normalized coordinates to absolute coordinates\n",
    "      abs_y1 = int(bounding_box[\"bbox_2d\"][1]/input_height * height)\n",
    "      abs_x1 = int(bounding_box[\"bbox_2d\"][0]/input_width * width)\n",
    "      abs_y2 = int(bounding_box[\"bbox_2d\"][3]/input_height * height)\n",
    "      abs_x2 = int(bounding_box[\"bbox_2d\"][2]/input_width * width)\n",
    "\n",
    "      if abs_x1 > abs_x2:\n",
    "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "      if abs_y1 > abs_y2:\n",
    "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "      # Draw the bounding box\n",
    "      draw.rectangle(\n",
    "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "      )\n",
    "\n",
    "      # Draw the text\n",
    "      if \"label\" in bounding_box:\n",
    "        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "\n",
    "def plot_points(im, text, input_width, input_height):\n",
    "  img = im\n",
    "  width, height = img.size\n",
    "  draw = ImageDraw.Draw(img)\n",
    "  colors = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple', 'brown', 'gray',\n",
    "    'beige', 'turquoise', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'teal',\n",
    "    'olive', 'coral', 'lavender', 'violet', 'gold', 'silver',\n",
    "  ] + additional_colors\n",
    "  xml_text = text.replace('```xml', '')\n",
    "  xml_text = xml_text.replace('```', '')\n",
    "  data = decode_xml_points(xml_text)\n",
    "  if data is None:\n",
    "    img.show()\n",
    "    return\n",
    "  points = data['points']\n",
    "  description = data['phrase']\n",
    "\n",
    "  font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
    "\n",
    "  for i, point in enumerate(points):\n",
    "    color = colors[i % len(colors)]\n",
    "    abs_x1 = int(point[0])/input_width * width\n",
    "    abs_y1 = int(point[1])/input_height * height\n",
    "    radius = 2\n",
    "    draw.ellipse([(abs_x1 - radius, abs_y1 - radius), (abs_x1 + radius, abs_y1 + radius)], fill=color)\n",
    "    draw.text((abs_x1 + 8, abs_y1 + 6), description, fill=color, font=font)\n",
    "  \n",
    "  img.show()\n",
    "  \n",
    "\n",
    "# @title Parsing JSON output\n",
    "def parse_json(json_output):\n",
    "    # Parsing out the markdown fencing\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
    "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
    "            break  # Exit the loop once \"```json\" is found\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img_url, prompt, system_prompt=\"You are a helpful assistant\", max_new_tokens=1024):\n",
    "  image = Image_PIL.open(img_url)\n",
    "  messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt\n",
    "        },\n",
    "        {\n",
    "          \"image\": img_url\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "  text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  print(\"input:\\n\",text)\n",
    "  inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "\n",
    "  # Patch if missing\n",
    "  if \"image_grid_thw\" not in inputs:\n",
    "      inputs[\"image_grid_thw\"] = torch.tensor([[14, 14, 1]], device='cuda')\n",
    "      \n",
    "  with torch.no_grad():\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "  \n",
    "  generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "  output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  print(\"output:\\n\",output_text[0])\n",
    "\n",
    "  input_height = inputs['image_grid_thw'][0][1]*14\n",
    "  input_width = inputs['image_grid_thw'][0][2]*14\n",
    "\n",
    "  return output_text[0], input_height, input_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: image/image_0.jpg\n",
      "✅ Saved: image/image_1.jpg\n",
      "✅ Saved: image/image_2.jpg\n",
      "✅ Saved: image/image_3.jpg\n",
      "✅ Saved: image/image_4.jpg\n",
      "✅ Saved: image/image_5.jpg\n",
      "✅ Saved: image/image_6.jpg\n",
      "✅ Saved: image/image_7.jpg\n",
      "✅ Saved: image/image_8.jpg\n",
      "✅ Saved: image/image_9.jpg\n",
      "✅ Saved: image/image_10.jpg\n",
      "✅ Saved: image/image_11.jpg\n",
      "✅ Saved: image/image_12.jpg\n",
      "✅ Saved: image/image_13.jpg\n",
      "✅ Saved: image/image_14.jpg\n",
      "✅ Saved: image/image_15.jpg\n",
      "✅ Saved: image/image_16.jpg\n",
      "✅ Saved: image/image_17.jpg\n",
      "✅ Saved: image/image_18.jpg\n",
      "✅ Saved: image/image_19.jpg\n",
      "✅ Saved: image/image_20.jpg\n",
      "✅ Saved: image/image_21.jpg\n",
      "✅ Saved: image/image_22.jpg\n",
      "✅ Saved: image/image_23.jpg\n",
      "✅ Saved: image/image_24.jpg\n",
      "✅ Saved: image/image_25.jpg\n",
      "✅ Saved: image/image_26.jpg\n",
      "✅ Saved: image/image_27.jpg\n",
      "✅ Saved: image/image_28.jpg\n",
      "✅ Saved: image/image_29.jpg\n",
      "✅ Saved: image/image_30.jpg\n",
      "✅ Saved: image/image_31.jpg\n",
      "✅ Saved: image/image_32.jpg\n",
      "✅ Saved: image/image_33.jpg\n",
      "✅ Saved: image/image_34.jpg\n",
      "✅ Saved: image/image_35.jpg\n",
      "✅ Saved: image/image_36.jpg\n",
      "✅ Saved: image/image_37.jpg\n",
      "✅ Saved: image/image_38.jpg\n",
      "✅ Saved: image/image_39.jpg\n",
      "✅ Saved: image/image_40.jpg\n",
      "✅ Saved: image/image_41.jpg\n",
      "✅ Saved: image/image_42.jpg\n",
      "✅ Saved: image/image_43.jpg\n",
      "✅ Saved: image/image_44.jpg\n",
      "✅ Saved: image/image_45.jpg\n",
      "✅ Saved: image/image_46.jpg\n",
      "✅ Saved: image/image_47.jpg\n",
      "✅ Saved: image/image_48.jpg\n",
      "✅ Saved: image/image_49.jpg\n",
      "✅ Saved: image/image_50.jpg\n",
      "✅ Saved: image/image_51.jpg\n",
      "✅ Saved: image/image_52.jpg\n",
      "✅ Saved: image/image_53.jpg\n",
      "✅ Saved: image/image_54.jpg\n",
      "✅ Saved: image/image_55.jpg\n",
      "✅ Saved: image/image_56.jpg\n",
      "✅ Saved: image/image_57.jpg\n",
      "✅ Saved: image/image_58.jpg\n",
      "✅ Saved: image/image_59.jpg\n",
      "✅ Saved: image/image_60.jpg\n",
      "✅ Saved: image/image_61.jpg\n",
      "✅ Saved: image/image_62.jpg\n",
      "✅ Saved: image/image_63.jpg\n",
      "✅ Saved: image/image_64.jpg\n",
      "✅ Saved: image/image_65.jpg\n",
      "✅ Saved: image/image_66.jpg\n",
      "✅ Saved: image/image_67.jpg\n",
      "✅ Saved: image/image_68.jpg\n",
      "✅ Saved: image/image_69.jpg\n",
      "✅ Saved: image/image_70.jpg\n",
      "✅ Saved: image/image_71.jpg\n",
      "✅ Saved: image/image_72.jpg\n",
      "✅ Saved: image/image_73.jpg\n",
      "✅ Saved: image/image_74.jpg\n",
      "✅ Saved: image/image_75.jpg\n",
      "✅ Saved: image/image_76.jpg\n",
      "✅ Saved: image/image_77.jpg\n",
      "✅ Saved: image/image_78.jpg\n",
      "✅ Saved: image/image_79.jpg\n",
      "✅ Saved: image/image_80.jpg\n",
      "✅ Saved: image/image_81.jpg\n",
      "✅ Saved: image/image_82.jpg\n",
      "✅ Saved: image/image_83.jpg\n",
      "✅ Saved: image/image_84.jpg\n",
      "✅ Saved: image/image_85.jpg\n",
      "✅ Saved: image/image_86.jpg\n",
      "✅ Saved: image/image_87.jpg\n",
      "✅ Saved: image/image_88.jpg\n",
      "✅ Saved: image/image_89.jpg\n",
      "✅ Saved: image/image_90.jpg\n",
      "✅ Saved: image/image_91.jpg\n",
      "✅ Saved: image/image_92.jpg\n",
      "✅ Saved: image/image_93.jpg\n",
      "✅ Saved: image/image_94.jpg\n",
      "✅ Saved: image/image_95.jpg\n",
      "✅ Saved: image/image_96.jpg\n",
      "✅ Saved: image/image_97.jpg\n",
      "✅ Saved: image/image_98.jpg\n",
      "✅ Saved: image/image_99.jpg\n",
      "✅ Saved: image/image_100.jpg\n",
      "✅ Saved: image/image_101.jpg\n",
      "✅ Saved: image/image_102.jpg\n",
      "✅ Saved: image/image_103.jpg\n",
      "✅ Saved: image/image_104.jpg\n",
      "✅ Saved: image/image_105.jpg\n",
      "✅ Saved: image/image_106.jpg\n",
      "✅ Saved: image/image_107.jpg\n",
      "✅ Saved: image/image_108.jpg\n",
      "✅ Saved: image/image_109.jpg\n",
      "✅ Saved: image/image_110.jpg\n",
      "✅ Saved: image/image_111.jpg\n",
      "✅ Saved: image/image_112.jpg\n",
      "✅ Saved: image/image_113.jpg\n",
      "✅ Saved: image/image_114.jpg\n",
      "✅ Saved: image/image_115.jpg\n",
      "✅ Saved: image/image_116.jpg\n",
      "✅ Saved: image/image_117.jpg\n",
      "✅ Saved: image/image_118.jpg\n",
      "✅ Saved: image/image_119.jpg\n",
      "✅ Saved: image/image_120.jpg\n",
      "✅ Saved: image/image_121.jpg\n",
      "✅ Saved: image/image_122.jpg\n",
      "✅ Saved: image/image_123.jpg\n",
      "✅ Saved: image/image_124.jpg\n",
      "✅ Saved: image/image_125.jpg\n",
      "✅ Saved: image/image_126.jpg\n",
      "✅ Saved: image/image_127.jpg\n",
      "✅ Saved: image/image_128.jpg\n",
      "✅ Saved: image/image_129.jpg\n",
      "✅ Saved: image/image_130.jpg\n",
      "✅ Saved: image/image_131.jpg\n",
      "✅ Saved: image/image_132.jpg\n",
      "✅ Saved: image/image_133.jpg\n",
      "✅ Saved: image/image_134.jpg\n",
      "✅ Saved: image/image_135.jpg\n",
      "✅ Saved: image/image_136.jpg\n",
      "✅ Saved: image/image_137.jpg\n",
      "✅ Saved: image/image_138.jpg\n",
      "✅ Saved: image/image_139.jpg\n",
      "✅ Saved: image/image_140.jpg\n",
      "✅ Saved: image/image_141.jpg\n",
      "✅ Saved: image/image_142.jpg\n",
      "✅ Saved: image/image_143.jpg\n",
      "✅ Saved: image/image_144.jpg\n",
      "✅ Saved: image/image_145.jpg\n",
      "✅ Saved: image/image_146.jpg\n",
      "✅ Saved: image/image_147.jpg\n",
      "✅ Saved: image/image_148.jpg\n",
      "✅ Saved: image/image_149.jpg\n",
      "✅ Saved: image/image_150.jpg\n",
      "✅ Saved: image/image_151.jpg\n",
      "✅ Saved: image/image_152.jpg\n",
      "✅ Saved: image/image_153.jpg\n",
      "✅ Saved: image/image_154.jpg\n",
      "✅ Saved: image/image_155.jpg\n",
      "✅ Saved: image/image_156.jpg\n",
      "✅ Saved: image/image_157.jpg\n",
      "✅ Saved: image/image_158.jpg\n",
      "✅ Saved: image/image_159.jpg\n",
      "✅ Saved: image/image_160.jpg\n",
      "✅ Saved: image/image_161.jpg\n",
      "✅ Saved: image/image_162.jpg\n",
      "✅ Saved: image/image_163.jpg\n",
      "✅ Saved: image/image_164.jpg\n",
      "✅ Saved: image/image_165.jpg\n",
      "✅ Saved: image/image_166.jpg\n",
      "✅ Saved: image/image_167.jpg\n",
      "✅ Saved: image/image_168.jpg\n",
      "✅ Saved: image/image_169.jpg\n",
      "✅ Saved: image/image_170.jpg\n",
      "✅ Saved: image/image_171.jpg\n",
      "✅ Saved: image/image_172.jpg\n",
      "✅ Saved: image/image_173.jpg\n",
      "✅ Saved: image/image_174.jpg\n",
      "✅ Saved: image/image_175.jpg\n",
      "✅ Saved: image/image_176.jpg\n",
      "✅ Saved: image/image_177.jpg\n",
      "✅ Saved: image/image_178.jpg\n",
      "✅ Saved: image/image_179.jpg\n",
      "✅ Saved: image/image_180.jpg\n",
      "✅ Saved: image/image_181.jpg\n",
      "✅ Saved: image/image_182.jpg\n",
      "✅ Saved: image/image_183.jpg\n",
      "✅ Saved: image/image_184.jpg\n",
      "✅ Saved: image/image_185.jpg\n",
      "✅ Saved: image/image_186.jpg\n",
      "✅ Saved: image/image_187.jpg\n",
      "✅ Saved: image/image_188.jpg\n",
      "✅ Saved: image/image_189.jpg\n",
      "✅ Saved: image/image_190.jpg\n",
      "✅ Saved: image/image_191.jpg\n",
      "✅ Saved: image/image_192.jpg\n",
      "✅ Saved: image/image_193.jpg\n",
      "✅ Saved: image/image_194.jpg\n",
      "✅ Saved: image/image_195.jpg\n",
      "✅ Saved: image/image_196.jpg\n",
      "✅ Saved: image/image_197.jpg\n",
      "✅ Saved: image/image_198.jpg\n",
      "✅ Saved: image/image_199.jpg\n",
      "✅ Saved: image/image_200.jpg\n",
      "✅ Saved: image/image_201.jpg\n",
      "✅ Saved: image/image_202.jpg\n",
      "✅ Saved: image/image_203.jpg\n",
      "✅ Saved: image/image_204.jpg\n",
      "✅ Saved: image/image_205.jpg\n",
      "✅ Saved: image/image_206.jpg\n",
      "✅ Saved: image/image_207.jpg\n",
      "✅ Saved: image/image_208.jpg\n",
      "✅ Saved: image/image_209.jpg\n",
      "✅ Saved: image/image_210.jpg\n",
      "✅ Saved: image/image_211.jpg\n",
      "✅ Saved: image/image_212.jpg\n",
      "✅ Saved: image/image_213.jpg\n",
      "✅ Saved: image/image_214.jpg\n",
      "✅ Saved: image/image_215.jpg\n",
      "✅ Saved: image/image_216.jpg\n",
      "✅ Saved: image/image_217.jpg\n",
      "✅ Saved: image/image_218.jpg\n",
      "✅ Saved: image/image_219.jpg\n",
      "✅ Saved: image/image_220.jpg\n",
      "✅ Saved: image/image_221.jpg\n",
      "✅ Saved: image/image_222.jpg\n",
      "✅ Saved: image/image_223.jpg\n",
      "✅ Saved: image/image_224.jpg\n",
      "✅ Saved: image/image_225.jpg\n",
      "✅ Saved: image/image_226.jpg\n",
      "✅ Saved: image/image_227.jpg\n",
      "✅ Saved: image/image_228.jpg\n",
      "✅ Saved: image/image_229.jpg\n",
      "✅ Saved: image/image_230.jpg\n",
      "✅ Saved: image/image_231.jpg\n",
      "✅ Saved: image/image_232.jpg\n",
      "✅ Saved: image/image_233.jpg\n",
      "✅ Saved: image/image_234.jpg\n",
      "✅ Saved: image/image_235.jpg\n",
      "✅ Saved: image/image_236.jpg\n",
      "✅ Saved: image/image_237.jpg\n",
      "✅ Saved: image/image_238.jpg\n",
      "✅ Saved: image/image_239.jpg\n",
      "✅ Saved: image/image_240.jpg\n",
      "✅ Saved: image/image_241.jpg\n",
      "✅ Saved: image/image_242.jpg\n",
      "✅ Saved: image/image_243.jpg\n",
      "✅ Saved: image/image_244.jpg\n",
      "✅ Saved: image/image_245.jpg\n",
      "✅ Saved: image/image_246.jpg\n",
      "✅ Saved: image/image_247.jpg\n",
      "✅ Saved: image/image_248.jpg\n",
      "✅ Saved: image/image_249.jpg\n",
      "✅ Saved: image/image_250.jpg\n",
      "✅ Saved: image/image_251.jpg\n",
      "✅ Saved: image/image_252.jpg\n",
      "✅ Saved: image/image_253.jpg\n",
      "✅ Saved: image/image_254.jpg\n",
      "✅ Saved: image/image_255.jpg\n",
      "✅ Saved: image/image_256.jpg\n",
      "✅ Saved: image/image_257.jpg\n",
      "✅ Saved: image/image_258.jpg\n",
      "✅ Saved: image/image_259.jpg\n",
      "✅ Saved: image/image_260.jpg\n",
      "✅ Saved: image/image_261.jpg\n",
      "✅ Saved: image/image_262.jpg\n",
      "✅ Saved: image/image_263.jpg\n",
      "✅ Saved: image/image_264.jpg\n",
      "✅ Saved: image/image_265.jpg\n",
      "✅ Saved: image/image_266.jpg\n",
      "✅ Saved: image/image_267.jpg\n",
      "✅ Saved: image/image_268.jpg\n",
      "✅ Saved: image/image_269.jpg\n",
      "✅ Saved: image/image_270.jpg\n",
      "✅ Saved: image/image_271.jpg\n",
      "✅ Saved: image/image_272.jpg\n",
      "✅ Saved: image/image_273.jpg\n",
      "✅ Saved: image/image_274.jpg\n",
      "✅ Saved: image/image_275.jpg\n",
      "✅ Saved: image/image_276.jpg\n",
      "✅ Saved: image/image_277.jpg\n",
      "✅ Saved: image/image_278.jpg\n",
      "✅ Saved: image/image_279.jpg\n",
      "✅ Saved: image/image_280.jpg\n",
      "✅ Saved: image/image_281.jpg\n",
      "✅ Saved: image/image_282.jpg\n",
      "✅ Saved: image/image_283.jpg\n",
      "✅ Saved: image/image_284.jpg\n",
      "✅ Saved: image/image_285.jpg\n",
      "✅ Saved: image/image_286.jpg\n",
      "✅ Saved: image/image_287.jpg\n",
      "✅ Saved: image/image_288.jpg\n",
      "✅ Saved: image/image_289.jpg\n",
      "✅ Saved: image/image_290.jpg\n",
      "✅ Saved: image/image_291.jpg\n",
      "✅ Saved: image/image_292.jpg\n",
      "✅ Saved: image/image_293.jpg\n",
      "✅ Saved: image/image_294.jpg\n",
      "✅ Saved: image/image_295.jpg\n",
      "✅ Saved: image/image_296.jpg\n",
      "✅ Saved: image/image_297.jpg\n",
      "✅ Saved: image/image_298.jpg\n",
      "✅ Saved: image/image_299.jpg\n",
      "✅ Saved: image/image_300.jpg\n",
      "✅ Saved: image/image_301.jpg\n",
      "✅ Saved: image/image_302.jpg\n",
      "✅ Saved: image/image_303.jpg\n",
      "✅ Saved: image/image_304.jpg\n",
      "✅ Saved: image/image_305.jpg\n",
      "✅ Saved: image/image_306.jpg\n",
      "✅ Saved: image/image_307.jpg\n",
      "✅ Saved: image/image_308.jpg\n",
      "✅ Saved: image/image_309.jpg\n",
      "✅ Saved: image/image_310.jpg\n",
      "✅ Saved: image/image_311.jpg\n",
      "✅ Saved: image/image_312.jpg\n",
      "✅ Saved: image/image_313.jpg\n",
      "✅ Saved: image/image_314.jpg\n",
      "✅ Saved: image/image_315.jpg\n",
      "✅ Saved: image/image_316.jpg\n",
      "✅ Saved: image/image_317.jpg\n",
      "✅ Saved: image/image_318.jpg\n",
      "✅ Saved: image/image_319.jpg\n",
      "✅ Saved: image/image_320.jpg\n",
      "✅ Saved: image/image_321.jpg\n",
      "✅ Saved: image/image_322.jpg\n",
      "✅ Saved: image/image_323.jpg\n",
      "✅ Saved: image/image_324.jpg\n",
      "✅ Saved: image/image_325.jpg\n",
      "✅ Saved: image/image_326.jpg\n",
      "✅ Saved: image/image_327.jpg\n",
      "✅ Saved: image/image_328.jpg\n",
      "✅ Saved: image/image_329.jpg\n",
      "✅ Saved: image/image_330.jpg\n",
      "✅ Saved: image/image_331.jpg\n",
      "✅ Saved: image/image_332.jpg\n",
      "✅ Saved: image/image_333.jpg\n",
      "✅ Saved: image/image_334.jpg\n",
      "✅ Saved: image/image_335.jpg\n",
      "✅ Saved: image/image_336.jpg\n",
      "✅ Saved: image/image_337.jpg\n",
      "✅ Saved: image/image_338.jpg\n",
      "✅ Saved: image/image_339.jpg\n",
      "✅ Saved: image/image_340.jpg\n",
      "✅ Saved: image/image_341.jpg\n",
      "✅ Saved: image/image_342.jpg\n",
      "✅ Saved: image/image_343.jpg\n",
      "✅ Saved: image/image_344.jpg\n",
      "✅ Saved: image/image_345.jpg\n",
      "✅ Saved: image/image_346.jpg\n",
      "✅ Saved: image/image_347.jpg\n",
      "✅ Saved: image/image_348.jpg\n",
      "✅ Saved: image/image_349.jpg\n",
      "✅ Saved: image/image_350.jpg\n",
      "✅ Saved: image/image_351.jpg\n",
      "✅ Saved: image/image_352.jpg\n",
      "✅ Saved: image/image_353.jpg\n",
      "✅ Saved: image/image_354.jpg\n",
      "✅ Saved: image/image_355.jpg\n",
      "✅ Saved: image/image_356.jpg\n",
      "✅ Saved: image/image_357.jpg\n",
      "✅ Saved: image/image_358.jpg\n",
      "✅ Saved: image/image_359.jpg\n",
      "✅ Saved: image/image_360.jpg\n",
      "✅ Saved: image/image_361.jpg\n",
      "✅ Saved: image/image_362.jpg\n",
      "✅ Saved: image/image_363.jpg\n",
      "✅ Saved: image/image_364.jpg\n",
      "✅ Saved: image/image_365.jpg\n",
      "✅ Saved: image/image_366.jpg\n",
      "✅ Saved: image/image_367.jpg\n",
      "✅ Saved: image/image_368.jpg\n",
      "✅ Saved: image/image_369.jpg\n",
      "✅ Saved: image/image_370.jpg\n",
      "✅ Saved: image/image_371.jpg\n",
      "✅ Saved: image/image_372.jpg\n",
      "✅ Saved: image/image_373.jpg\n",
      "✅ Saved: image/image_374.jpg\n",
      "✅ Saved: image/image_375.jpg\n",
      "✅ Saved: image/image_376.jpg\n",
      "✅ Saved: image/image_377.jpg\n",
      "✅ Saved: image/image_378.jpg\n",
      "✅ Saved: image/image_379.jpg\n",
      "✅ Saved: image/image_380.jpg\n",
      "✅ Saved: image/image_381.jpg\n",
      "✅ Saved: image/image_382.jpg\n",
      "✅ Saved: image/image_383.jpg\n",
      "✅ Saved: image/image_384.jpg\n",
      "✅ Saved: image/image_385.jpg\n",
      "✅ Saved: image/image_386.jpg\n",
      "✅ Saved: image/image_387.jpg\n",
      "✅ Saved: image/image_388.jpg\n",
      "✅ Saved: image/image_389.jpg\n",
      "✅ Saved: image/image_390.jpg\n",
      "✅ Saved: image/image_391.jpg\n",
      "✅ Saved: image/image_392.jpg\n",
      "✅ Saved: image/image_393.jpg\n",
      "✅ Saved: image/image_394.jpg\n",
      "✅ Saved: image/image_395.jpg\n",
      "✅ Saved: image/image_396.jpg\n",
      "✅ Saved: image/image_397.jpg\n",
      "✅ Saved: image/image_398.jpg\n",
      "✅ Saved: image/image_399.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image as Image_PIL\n",
    "import shutil\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "hf_user = \"Rajarshi-Roy-research\"\n",
    "# dataset_name = \"Flickr30k_Grounding_Som\"\n",
    "# commit_message = \"Updating dataset with image paths\"\n",
    "# img_cols = [\"image\",\"wbox_image\"]\n",
    "# image_to_process = \"image\"\n",
    "\n",
    "dataset_name = \"refcocog-eval-depth\"\n",
    "commit_message = \"Updating dataset with images and depth caption\"\n",
    "img_cols = [\"image\"]\n",
    "image_to_process = \"image\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_image_from_bytes(image_data, idx, image_folder):\n",
    "    \"\"\"Saves an image from byte data and returns the saved file path.\"\"\"\n",
    "    try:\n",
    "        image_bytes = image_data.get(\"bytes\", None)\n",
    "        if image_bytes:\n",
    "            image = Image_PIL.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(image_folder, f\"image_{idx}.jpg\")\n",
    "            image.save(image_path, format=\"JPEG\")\n",
    "            print(f\"✅ Saved: {image_path}\")\n",
    "            return image_path\n",
    "        else:\n",
    "            print(f\"⚠️ Skipped row {idx}: No bytes found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving image at row {idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "# Clean up existing image folder\n",
    "for img_col in img_cols:\n",
    "    if os.path.exists(img_col):\n",
    "        shutil.rmtree(img_col)\n",
    "    os.makedirs(img_col)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(f\"{hf_user}/{dataset_name}\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "for img_col in img_cols:\n",
    "    # Save images and update DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if isinstance(row[img_col], dict):\n",
    "            df.at[i, img_col] = save_image_from_bytes(row[img_col], i, img_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>bbox</th>\n",
       "      <th>bbox_area</th>\n",
       "      <th>bbox_id</th>\n",
       "      <th>ori_category_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>file_name</th>\n",
       "      <th>is_rewrite</th>\n",
       "      <th>split</th>\n",
       "      <th>image</th>\n",
       "      <th>depth_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4381</td>\n",
       "      <td>A pair of tortillas containing a visible filli...</td>\n",
       "      <td>[300.25585936 239.42510986 183.92150884 133.25...</td>\n",
       "      <td>24508.697297</td>\n",
       "      <td>o365_24196196</td>\n",
       "      <td>o365_172</td>\n",
       "      <td>o365_489077</td>\n",
       "      <td>512</td>\n",
       "      <td>769</td>\n",
       "      <td>sampled_images/objects365_v1_00489077.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_0.jpg</td>\n",
       "      <td>Closest: a couple\\n----\\nMid Range: a bar\\n---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3385</td>\n",
       "      <td>The closed-top, white bucket-shaped item, posi...</td>\n",
       "      <td>[647.26123049 556.64379886  98.25061038 106.61...</td>\n",
       "      <td>10474.736003</td>\n",
       "      <td>o365_3289057</td>\n",
       "      <td>o365_49</td>\n",
       "      <td>o365_1040341</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_01040341.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_1.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: tents\\n----\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27674</td>\n",
       "      <td>The pink and white surfboard is curretnly ride...</td>\n",
       "      <td>[120.05 163.64 232.97  77.18]</td>\n",
       "      <td>17980.624600</td>\n",
       "      <td>coco_650459</td>\n",
       "      <td>refcoco_42</td>\n",
       "      <td>coco_449136</td>\n",
       "      <td>318</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000449136.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_2.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: a surfer\\n---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44294</td>\n",
       "      <td>The man wearing a cheerful Santa hat, characte...</td>\n",
       "      <td>[240.81  14.21 275.38 370.15]</td>\n",
       "      <td>101931.907000</td>\n",
       "      <td>coco_2165449</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_242583</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000242583.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_3.jpg</td>\n",
       "      <td>Closest: a man\\n----\\nMid Range: a hand\\n----\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29717</td>\n",
       "      <td>The pizza adorned with chunks of white topping...</td>\n",
       "      <td>[143.32 283.76 353.97 139.47]</td>\n",
       "      <td>49368.195900</td>\n",
       "      <td>coco_1074038</td>\n",
       "      <td>refcoco_59</td>\n",
       "      <td>coco_70415</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000070415.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_4.jpg</td>\n",
       "      <td>Closest: a pizza\\n----\\nMid Range: a person\\n-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>12011</td>\n",
       "      <td>The green purse hanging on the wall close to t...</td>\n",
       "      <td>[232.43835453 416.28540035  57.75231928  61.99...</td>\n",
       "      <td>3580.129160</td>\n",
       "      <td>o365_27275769</td>\n",
       "      <td>o365_14</td>\n",
       "      <td>o365_2053317</td>\n",
       "      <td>982</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_02053317.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_395.jpg</td>\n",
       "      <td>Closest: two motorcycles\\n----\\nMid Range: a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>36631</td>\n",
       "      <td>The man standing in the background, behind the...</td>\n",
       "      <td>[363.87 211.18 116.49 306.33]</td>\n",
       "      <td>35684.381700</td>\n",
       "      <td>coco_500529</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_242213</td>\n",
       "      <td>640</td>\n",
       "      <td>512</td>\n",
       "      <td>sampled_images/COCO_train2014_000000242213.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_396.jpg</td>\n",
       "      <td>Closest: a baseball pitcher\\n----\\nMid Range: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>18231</td>\n",
       "      <td>The skier clad in a red jacket is standing wit...</td>\n",
       "      <td>[474.02 105.55 130.5  316.65]</td>\n",
       "      <td>41322.825000</td>\n",
       "      <td>coco_512212</td>\n",
       "      <td>refcoco_1</td>\n",
       "      <td>coco_271641</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>sampled_images/COCO_train2014_000000271641.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_397.jpg</td>\n",
       "      <td>Closest: people\\n----\\nMid Range: a silhouette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>5846</td>\n",
       "      <td>A transparent drinking vessel containing a yel...</td>\n",
       "      <td>[284.04260255 188.13116452 100.05944822 199.01...</td>\n",
       "      <td>19913.766167</td>\n",
       "      <td>o365_9095135</td>\n",
       "      <td>o365_11</td>\n",
       "      <td>o365_476705</td>\n",
       "      <td>683</td>\n",
       "      <td>512</td>\n",
       "      <td>sampled_images/objects365_v1_00476705.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_398.jpg</td>\n",
       "      <td>Closest: candles\\n----\\nMid Range: a plate of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>16412</td>\n",
       "      <td>A tamed ruminant creature is nestled in the st...</td>\n",
       "      <td>[552.77685545 385.09814454 197.29370122 125.13...</td>\n",
       "      <td>24687.747138</td>\n",
       "      <td>o365_18759141</td>\n",
       "      <td>o365_100</td>\n",
       "      <td>o365_1693044</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>sampled_images/objects365_v2_01693044.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>image/image_399.jpg</td>\n",
       "      <td>Closest: a car\\n----\\nMid Range: a car\\n----\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            caption  \\\n",
       "0     4381  A pair of tortillas containing a visible filli...   \n",
       "1     3385  The closed-top, white bucket-shaped item, posi...   \n",
       "2    27674  The pink and white surfboard is curretnly ride...   \n",
       "3    44294  The man wearing a cheerful Santa hat, characte...   \n",
       "4    29717  The pizza adorned with chunks of white topping...   \n",
       "..     ...                                                ...   \n",
       "395  12011  The green purse hanging on the wall close to t...   \n",
       "396  36631  The man standing in the background, behind the...   \n",
       "397  18231  The skier clad in a red jacket is standing wit...   \n",
       "398   5846  A transparent drinking vessel containing a yel...   \n",
       "399  16412  A tamed ruminant creature is nestled in the st...   \n",
       "\n",
       "                                                  bbox      bbox_area  \\\n",
       "0    [300.25585936 239.42510986 183.92150884 133.25...   24508.697297   \n",
       "1    [647.26123049 556.64379886  98.25061038 106.61...   10474.736003   \n",
       "2                        [120.05 163.64 232.97  77.18]   17980.624600   \n",
       "3                        [240.81  14.21 275.38 370.15]  101931.907000   \n",
       "4                        [143.32 283.76 353.97 139.47]   49368.195900   \n",
       "..                                                 ...            ...   \n",
       "395  [232.43835453 416.28540035  57.75231928  61.99...    3580.129160   \n",
       "396                      [363.87 211.18 116.49 306.33]   35684.381700   \n",
       "397                      [474.02 105.55 130.5  316.65]   41322.825000   \n",
       "398  [284.04260255 188.13116452 100.05944822 199.01...   19913.766167   \n",
       "399  [552.77685545 385.09814454 197.29370122 125.13...   24687.747138   \n",
       "\n",
       "           bbox_id ori_category_id      image_id  height  width  \\\n",
       "0    o365_24196196        o365_172   o365_489077     512    769   \n",
       "1     o365_3289057         o365_49  o365_1040341     768   1024   \n",
       "2      coco_650459      refcoco_42   coco_449136     318    640   \n",
       "3     coco_2165449       refcoco_1   coco_242583     427    640   \n",
       "4     coco_1074038      refcoco_59    coco_70415     429    640   \n",
       "..             ...             ...           ...     ...    ...   \n",
       "395  o365_27275769         o365_14  o365_2053317     982   1024   \n",
       "396    coco_500529       refcoco_1   coco_242213     640    512   \n",
       "397    coco_512212       refcoco_1   coco_271641     427    640   \n",
       "398   o365_9095135         o365_11   o365_476705     683    512   \n",
       "399  o365_18759141        o365_100  o365_1693044     768   1024   \n",
       "\n",
       "                                          file_name  is_rewrite split  \\\n",
       "0         sampled_images/objects365_v1_00489077.jpg        True   val   \n",
       "1         sampled_images/objects365_v2_01040341.jpg        True   val   \n",
       "2    sampled_images/COCO_train2014_000000449136.jpg        True   val   \n",
       "3    sampled_images/COCO_train2014_000000242583.jpg        True   val   \n",
       "4    sampled_images/COCO_train2014_000000070415.jpg       False   val   \n",
       "..                                              ...         ...   ...   \n",
       "395       sampled_images/objects365_v2_02053317.jpg        True   val   \n",
       "396  sampled_images/COCO_train2014_000000242213.jpg       False   val   \n",
       "397  sampled_images/COCO_train2014_000000271641.jpg       False   val   \n",
       "398       sampled_images/objects365_v1_00476705.jpg       False   val   \n",
       "399       sampled_images/objects365_v2_01693044.jpg        True   val   \n",
       "\n",
       "                   image                                      depth_caption  \n",
       "0      image/image_0.jpg  Closest: a couple\\n----\\nMid Range: a bar\\n---...  \n",
       "1      image/image_1.jpg  Closest: a man\\n----\\nMid Range: tents\\n----\\n...  \n",
       "2      image/image_2.jpg  Closest: a man\\n----\\nMid Range: a surfer\\n---...  \n",
       "3      image/image_3.jpg  Closest: a man\\n----\\nMid Range: a hand\\n----\\...  \n",
       "4      image/image_4.jpg  Closest: a pizza\\n----\\nMid Range: a person\\n-...  \n",
       "..                   ...                                                ...  \n",
       "395  image/image_395.jpg  Closest: two motorcycles\\n----\\nMid Range: a m...  \n",
       "396  image/image_396.jpg  Closest: a baseball pitcher\\n----\\nMid Range: ...  \n",
       "397  image/image_397.jpg  Closest: people\\n----\\nMid Range: a silhouette...  \n",
       "398  image/image_398.jpg  Closest: candles\\n----\\nMid Range: a plate of ...  \n",
       "399  image/image_399.jpg  Closest: a car\\n----\\nMid Range: a car\\n----\\n...  \n",
       "\n",
       "[400 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image/image_0.jpg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Patch if missing\n",
    "if \"image_grid_thw\" not in inputs:\n",
    "    inputs[\"image_grid_thw\"] = torch.tensor([[14, 14, 1]], device='cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " <|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Outline the position of:\n",
      " A pair of tortillas containing a visible filling are positioned next to a heap of rice on a round plate.\n",
      "and output all the coordinates in JSON format.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutline the position of each small cake and output all the coordinates in JSON format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mOutline the position of:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mand output all the coordinates in JSON format.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m response, input_height, input_width \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m Image_PIL\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39msize)\n",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m, in \u001b[0;36minference\u001b[0;34m(img_url, prompt, system_prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m   output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, output_ids)]\n\u001b[1;32m     35\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3286\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3283\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3286\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1793\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1792\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1793\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m     n_image_tokens \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1795\u001b[0m     n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:557\u001b[0m, in \u001b[0;36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m    553\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    554\u001b[0m             blk\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cu_seqlens_now, \u001b[38;5;28;01mNone\u001b[39;00m, position_embeddings\n\u001b[1;32m    555\u001b[0m         )\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(hidden_states)\n\u001b[1;32m    560\u001b[0m reverse_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(window_index)\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:350\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 350\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:200\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionFlashAttention2.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 200\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb_flashatt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    202\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Depth-SoM/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:167\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb_flashatt\u001b[0;34m(q, k, cos, sin)\u001b[0m\n\u001b[1;32m    165\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    166\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 167\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(q)\n\u001b[1;32m    168\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m apply_rotary_emb(k\u001b[38;5;241m.\u001b[39mfloat(), cos\u001b[38;5;241m.\u001b[39mfloat(), sin\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mtype_as(k)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "image_path = df[\"image\"][idx]\n",
    "\n",
    "\n",
    "## Use a local HuggingFace model to inference.\n",
    "# prompt in chinese\n",
    "prompt = \"框出每一个小蛋糕的位置，以json格式输出所有的坐标\"\n",
    "# prompt in english\n",
    "prompt = \"Outline the position of each small cake and output all the coordinates in JSON format.\"\n",
    "\n",
    "prompt = f\"\"\"Outline the position of:\\n {df[\"caption\"][idx]}\\nand output all the coordinates in JSON format.\"\"\"\n",
    "response, input_height, input_width = inference(image_path, prompt)\n",
    "\n",
    "image = Image_PIL.open(image_path)\n",
    "print(image.size)\n",
    "image.thumbnail([640,640], Image_PIL.Resampling.LANCZOS)\n",
    "plot_bounding_boxes(image,response,input_width,input_height)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
